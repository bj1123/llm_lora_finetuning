{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9999040412835722,
  "eval_steps": 500,
  "global_step": 8792,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0011372884909958737,
      "grad_norm": 1.2493418455123901,
      "learning_rate": 0.0002996929026387625,
      "loss": 2.1948,
      "step": 10
    },
    {
      "epoch": 0.0022745769819917474,
      "grad_norm": 0.6513676643371582,
      "learning_rate": 0.0002993516833484986,
      "loss": 1.7783,
      "step": 20
    },
    {
      "epoch": 0.0034118654729876213,
      "grad_norm": 0.6632276177406311,
      "learning_rate": 0.0002990104640582347,
      "loss": 1.7081,
      "step": 30
    },
    {
      "epoch": 0.004549153963983495,
      "grad_norm": 0.6960775256156921,
      "learning_rate": 0.00029866924476797086,
      "loss": 1.6752,
      "step": 40
    },
    {
      "epoch": 0.005686442454979369,
      "grad_norm": 0.6987755298614502,
      "learning_rate": 0.000298328025477707,
      "loss": 1.7194,
      "step": 50
    },
    {
      "epoch": 0.006823730945975243,
      "grad_norm": 0.5958120226860046,
      "learning_rate": 0.0002979868061874431,
      "loss": 1.6596,
      "step": 60
    },
    {
      "epoch": 0.007961019436971117,
      "grad_norm": 0.6100614666938782,
      "learning_rate": 0.0002976455868971792,
      "loss": 1.6432,
      "step": 70
    },
    {
      "epoch": 0.00909830792796699,
      "grad_norm": 0.61018306016922,
      "learning_rate": 0.00029730436760691537,
      "loss": 1.6809,
      "step": 80
    },
    {
      "epoch": 0.010235596418962864,
      "grad_norm": 0.6855601072311401,
      "learning_rate": 0.00029696314831665146,
      "loss": 1.6444,
      "step": 90
    },
    {
      "epoch": 0.011372884909958738,
      "grad_norm": 0.6825836300849915,
      "learning_rate": 0.0002966219290263876,
      "loss": 1.7535,
      "step": 100
    },
    {
      "epoch": 0.012510173400954611,
      "grad_norm": 0.683988630771637,
      "learning_rate": 0.0002962807097361237,
      "loss": 1.6258,
      "step": 110
    },
    {
      "epoch": 0.013647461891950485,
      "grad_norm": 0.6677632927894592,
      "learning_rate": 0.0002959394904458598,
      "loss": 1.6691,
      "step": 120
    },
    {
      "epoch": 0.01478475038294636,
      "grad_norm": 0.7540801763534546,
      "learning_rate": 0.00029559827115559597,
      "loss": 1.6426,
      "step": 130
    },
    {
      "epoch": 0.015922038873942234,
      "grad_norm": 0.6357186436653137,
      "learning_rate": 0.0002952570518653321,
      "loss": 1.6151,
      "step": 140
    },
    {
      "epoch": 0.017059327364938107,
      "grad_norm": 0.7878345251083374,
      "learning_rate": 0.0002949158325750682,
      "loss": 1.6886,
      "step": 150
    },
    {
      "epoch": 0.01819661585593398,
      "grad_norm": 0.7538363337516785,
      "learning_rate": 0.0002945746132848043,
      "loss": 1.6052,
      "step": 160
    },
    {
      "epoch": 0.019333904346929855,
      "grad_norm": 0.7103121876716614,
      "learning_rate": 0.00029423339399454047,
      "loss": 1.5986,
      "step": 170
    },
    {
      "epoch": 0.020471192837925728,
      "grad_norm": 0.7904124855995178,
      "learning_rate": 0.00029389217470427657,
      "loss": 1.599,
      "step": 180
    },
    {
      "epoch": 0.0216084813289216,
      "grad_norm": 0.7235739827156067,
      "learning_rate": 0.0002935509554140127,
      "loss": 1.644,
      "step": 190
    },
    {
      "epoch": 0.022745769819917477,
      "grad_norm": 0.6870411038398743,
      "learning_rate": 0.0002932097361237488,
      "loss": 1.595,
      "step": 200
    },
    {
      "epoch": 0.02388305831091335,
      "grad_norm": 0.6266641616821289,
      "learning_rate": 0.000292868516833485,
      "loss": 1.6483,
      "step": 210
    },
    {
      "epoch": 0.025020346801909222,
      "grad_norm": 0.644254744052887,
      "learning_rate": 0.00029252729754322107,
      "loss": 1.5827,
      "step": 220
    },
    {
      "epoch": 0.026157635292905098,
      "grad_norm": 0.7717599868774414,
      "learning_rate": 0.0002921860782529572,
      "loss": 1.5829,
      "step": 230
    },
    {
      "epoch": 0.02729492378390097,
      "grad_norm": 0.6786218881607056,
      "learning_rate": 0.0002918448589626933,
      "loss": 1.6867,
      "step": 240
    },
    {
      "epoch": 0.028432212274896843,
      "grad_norm": 0.7582827210426331,
      "learning_rate": 0.0002915036396724295,
      "loss": 1.6053,
      "step": 250
    },
    {
      "epoch": 0.02956950076589272,
      "grad_norm": 0.6701986193656921,
      "learning_rate": 0.0002911624203821656,
      "loss": 1.6174,
      "step": 260
    },
    {
      "epoch": 0.030706789256888592,
      "grad_norm": 0.7445237040519714,
      "learning_rate": 0.0002908212010919017,
      "loss": 1.6102,
      "step": 270
    },
    {
      "epoch": 0.03184407774788447,
      "grad_norm": 0.8214905261993408,
      "learning_rate": 0.00029047998180163783,
      "loss": 1.5885,
      "step": 280
    },
    {
      "epoch": 0.03298136623888034,
      "grad_norm": 0.6089838147163391,
      "learning_rate": 0.000290138762511374,
      "loss": 1.6118,
      "step": 290
    },
    {
      "epoch": 0.03411865472987621,
      "grad_norm": 0.6585022807121277,
      "learning_rate": 0.0002897975432211101,
      "loss": 1.5772,
      "step": 300
    },
    {
      "epoch": 0.03525594322087209,
      "grad_norm": 0.6482012271881104,
      "learning_rate": 0.0002894563239308462,
      "loss": 1.6124,
      "step": 310
    },
    {
      "epoch": 0.03639323171186796,
      "grad_norm": 0.6968353390693665,
      "learning_rate": 0.00028911510464058233,
      "loss": 1.5441,
      "step": 320
    },
    {
      "epoch": 0.037530520202863835,
      "grad_norm": 0.668904185295105,
      "learning_rate": 0.00028877388535031843,
      "loss": 1.617,
      "step": 330
    },
    {
      "epoch": 0.03866780869385971,
      "grad_norm": 0.6475210189819336,
      "learning_rate": 0.0002884326660600546,
      "loss": 1.5875,
      "step": 340
    },
    {
      "epoch": 0.03980509718485558,
      "grad_norm": 0.6425984501838684,
      "learning_rate": 0.0002880914467697907,
      "loss": 1.601,
      "step": 350
    },
    {
      "epoch": 0.040942385675851456,
      "grad_norm": 0.6410463452339172,
      "learning_rate": 0.0002877502274795268,
      "loss": 1.5911,
      "step": 360
    },
    {
      "epoch": 0.04207967416684733,
      "grad_norm": 0.7064507603645325,
      "learning_rate": 0.00028740900818926293,
      "loss": 1.5687,
      "step": 370
    },
    {
      "epoch": 0.0432169626578432,
      "grad_norm": 0.6484823226928711,
      "learning_rate": 0.0002870677888989991,
      "loss": 1.5761,
      "step": 380
    },
    {
      "epoch": 0.04435425114883908,
      "grad_norm": 0.6280230283737183,
      "learning_rate": 0.0002867265696087352,
      "loss": 1.5876,
      "step": 390
    },
    {
      "epoch": 0.04549153963983495,
      "grad_norm": 0.6495710611343384,
      "learning_rate": 0.0002863853503184713,
      "loss": 1.6318,
      "step": 400
    },
    {
      "epoch": 0.04662882813083082,
      "grad_norm": 0.6800521016120911,
      "learning_rate": 0.00028604413102820744,
      "loss": 1.5575,
      "step": 410
    },
    {
      "epoch": 0.0477661166218267,
      "grad_norm": 0.6117984652519226,
      "learning_rate": 0.0002857029117379436,
      "loss": 1.631,
      "step": 420
    },
    {
      "epoch": 0.048903405112822575,
      "grad_norm": 0.6547871828079224,
      "learning_rate": 0.0002853616924476797,
      "loss": 1.644,
      "step": 430
    },
    {
      "epoch": 0.050040693603818444,
      "grad_norm": 0.685017466545105,
      "learning_rate": 0.0002850204731574158,
      "loss": 1.6312,
      "step": 440
    },
    {
      "epoch": 0.05117798209481432,
      "grad_norm": 0.6671081185340881,
      "learning_rate": 0.00028467925386715194,
      "loss": 1.6025,
      "step": 450
    },
    {
      "epoch": 0.052315270585810196,
      "grad_norm": 0.7380251288414001,
      "learning_rate": 0.00028433803457688804,
      "loss": 1.6102,
      "step": 460
    },
    {
      "epoch": 0.053452559076806065,
      "grad_norm": 0.6076188683509827,
      "learning_rate": 0.0002839968152866242,
      "loss": 1.5925,
      "step": 470
    },
    {
      "epoch": 0.05458984756780194,
      "grad_norm": 0.7031903266906738,
      "learning_rate": 0.0002836555959963603,
      "loss": 1.6481,
      "step": 480
    },
    {
      "epoch": 0.05572713605879782,
      "grad_norm": 0.6482532620429993,
      "learning_rate": 0.00028331437670609644,
      "loss": 1.5822,
      "step": 490
    },
    {
      "epoch": 0.056864424549793686,
      "grad_norm": 0.6127076745033264,
      "learning_rate": 0.00028297315741583254,
      "loss": 1.5553,
      "step": 500
    },
    {
      "epoch": 0.05800171304078956,
      "grad_norm": 0.6483978629112244,
      "learning_rate": 0.00028263193812556864,
      "loss": 1.6073,
      "step": 510
    },
    {
      "epoch": 0.05913900153178544,
      "grad_norm": 0.6152169108390808,
      "learning_rate": 0.0002822907188353048,
      "loss": 1.5512,
      "step": 520
    },
    {
      "epoch": 0.06027629002278131,
      "grad_norm": 0.7126293182373047,
      "learning_rate": 0.00028194949954504095,
      "loss": 1.6292,
      "step": 530
    },
    {
      "epoch": 0.061413578513777184,
      "grad_norm": 0.7340486645698547,
      "learning_rate": 0.00028160828025477705,
      "loss": 1.5523,
      "step": 540
    },
    {
      "epoch": 0.06255086700477305,
      "grad_norm": 0.7415356040000916,
      "learning_rate": 0.00028126706096451314,
      "loss": 1.5999,
      "step": 550
    },
    {
      "epoch": 0.06368815549576894,
      "grad_norm": 0.5884100794792175,
      "learning_rate": 0.0002809258416742493,
      "loss": 1.5355,
      "step": 560
    },
    {
      "epoch": 0.0648254439867648,
      "grad_norm": 0.6547891497612,
      "learning_rate": 0.00028058462238398545,
      "loss": 1.581,
      "step": 570
    },
    {
      "epoch": 0.06596273247776067,
      "grad_norm": 0.622607946395874,
      "learning_rate": 0.00028024340309372155,
      "loss": 1.5801,
      "step": 580
    },
    {
      "epoch": 0.06710002096875656,
      "grad_norm": 0.6480998992919922,
      "learning_rate": 0.00027990218380345765,
      "loss": 1.5742,
      "step": 590
    },
    {
      "epoch": 0.06823730945975243,
      "grad_norm": 0.7648559808731079,
      "learning_rate": 0.0002795609645131938,
      "loss": 1.5682,
      "step": 600
    },
    {
      "epoch": 0.0693745979507483,
      "grad_norm": 0.6642588973045349,
      "learning_rate": 0.0002792197452229299,
      "loss": 1.5744,
      "step": 610
    },
    {
      "epoch": 0.07051188644174418,
      "grad_norm": 0.599215030670166,
      "learning_rate": 0.00027887852593266605,
      "loss": 1.5923,
      "step": 620
    },
    {
      "epoch": 0.07164917493274005,
      "grad_norm": 0.6313246488571167,
      "learning_rate": 0.00027853730664240215,
      "loss": 1.5312,
      "step": 630
    },
    {
      "epoch": 0.07278646342373592,
      "grad_norm": 0.6710538268089294,
      "learning_rate": 0.00027819608735213825,
      "loss": 1.5929,
      "step": 640
    },
    {
      "epoch": 0.0739237519147318,
      "grad_norm": 0.6599650979042053,
      "learning_rate": 0.0002778548680618744,
      "loss": 1.6013,
      "step": 650
    },
    {
      "epoch": 0.07506104040572767,
      "grad_norm": 0.6440061330795288,
      "learning_rate": 0.00027751364877161056,
      "loss": 1.5874,
      "step": 660
    },
    {
      "epoch": 0.07619832889672354,
      "grad_norm": 0.7527706027030945,
      "learning_rate": 0.00027717242948134665,
      "loss": 1.52,
      "step": 670
    },
    {
      "epoch": 0.07733561738771942,
      "grad_norm": 0.6381344795227051,
      "learning_rate": 0.00027683121019108275,
      "loss": 1.5343,
      "step": 680
    },
    {
      "epoch": 0.07847290587871529,
      "grad_norm": 0.6032513380050659,
      "learning_rate": 0.0002764899909008189,
      "loss": 1.6183,
      "step": 690
    },
    {
      "epoch": 0.07961019436971116,
      "grad_norm": 0.6491003036499023,
      "learning_rate": 0.00027614877161055506,
      "loss": 1.5588,
      "step": 700
    },
    {
      "epoch": 0.08074748286070704,
      "grad_norm": 0.6983680725097656,
      "learning_rate": 0.00027580755232029116,
      "loss": 1.542,
      "step": 710
    },
    {
      "epoch": 0.08188477135170291,
      "grad_norm": 0.7331786751747131,
      "learning_rate": 0.00027546633303002726,
      "loss": 1.5803,
      "step": 720
    },
    {
      "epoch": 0.08302205984269878,
      "grad_norm": 0.6466768383979797,
      "learning_rate": 0.0002751251137397634,
      "loss": 1.6083,
      "step": 730
    },
    {
      "epoch": 0.08415934833369466,
      "grad_norm": 0.5665150284767151,
      "learning_rate": 0.0002747838944494995,
      "loss": 1.5774,
      "step": 740
    },
    {
      "epoch": 0.08529663682469053,
      "grad_norm": 0.7088013887405396,
      "learning_rate": 0.00027444267515923566,
      "loss": 1.552,
      "step": 750
    },
    {
      "epoch": 0.0864339253156864,
      "grad_norm": 0.5949841737747192,
      "learning_rate": 0.00027410145586897176,
      "loss": 1.6126,
      "step": 760
    },
    {
      "epoch": 0.08757121380668229,
      "grad_norm": 0.5945487022399902,
      "learning_rate": 0.0002737602365787079,
      "loss": 1.5145,
      "step": 770
    },
    {
      "epoch": 0.08870850229767815,
      "grad_norm": 0.6318658590316772,
      "learning_rate": 0.000273419017288444,
      "loss": 1.5273,
      "step": 780
    },
    {
      "epoch": 0.08984579078867402,
      "grad_norm": 0.6297826170921326,
      "learning_rate": 0.0002730777979981801,
      "loss": 1.5576,
      "step": 790
    },
    {
      "epoch": 0.0909830792796699,
      "grad_norm": 0.6225904822349548,
      "learning_rate": 0.00027273657870791626,
      "loss": 1.5248,
      "step": 800
    },
    {
      "epoch": 0.09212036777066578,
      "grad_norm": 0.5915766358375549,
      "learning_rate": 0.0002723953594176524,
      "loss": 1.5752,
      "step": 810
    },
    {
      "epoch": 0.09325765626166165,
      "grad_norm": 0.6827594637870789,
      "learning_rate": 0.0002720541401273885,
      "loss": 1.6019,
      "step": 820
    },
    {
      "epoch": 0.09439494475265753,
      "grad_norm": 0.6382894515991211,
      "learning_rate": 0.0002717129208371246,
      "loss": 1.6031,
      "step": 830
    },
    {
      "epoch": 0.0955322332436534,
      "grad_norm": 0.7003089189529419,
      "learning_rate": 0.00027137170154686077,
      "loss": 1.5988,
      "step": 840
    },
    {
      "epoch": 0.09666952173464927,
      "grad_norm": 0.6110066771507263,
      "learning_rate": 0.0002710304822565969,
      "loss": 1.5156,
      "step": 850
    },
    {
      "epoch": 0.09780681022564515,
      "grad_norm": 0.6973838210105896,
      "learning_rate": 0.000270689262966333,
      "loss": 1.5859,
      "step": 860
    },
    {
      "epoch": 0.09894409871664102,
      "grad_norm": 0.6852993965148926,
      "learning_rate": 0.0002703480436760691,
      "loss": 1.5981,
      "step": 870
    },
    {
      "epoch": 0.10008138720763689,
      "grad_norm": 0.62818843126297,
      "learning_rate": 0.00027000682438580527,
      "loss": 1.5047,
      "step": 880
    },
    {
      "epoch": 0.10121867569863277,
      "grad_norm": 0.6086484789848328,
      "learning_rate": 0.00026966560509554137,
      "loss": 1.539,
      "step": 890
    },
    {
      "epoch": 0.10235596418962864,
      "grad_norm": 0.6531029939651489,
      "learning_rate": 0.0002693243858052775,
      "loss": 1.5867,
      "step": 900
    },
    {
      "epoch": 0.10349325268062451,
      "grad_norm": 0.756874144077301,
      "learning_rate": 0.0002689831665150136,
      "loss": 1.5432,
      "step": 910
    },
    {
      "epoch": 0.10463054117162039,
      "grad_norm": 0.6879647970199585,
      "learning_rate": 0.0002686419472247497,
      "loss": 1.5333,
      "step": 920
    },
    {
      "epoch": 0.10576782966261626,
      "grad_norm": 0.6772540211677551,
      "learning_rate": 0.00026830072793448587,
      "loss": 1.5552,
      "step": 930
    },
    {
      "epoch": 0.10690511815361213,
      "grad_norm": 0.6537169814109802,
      "learning_rate": 0.000267959508644222,
      "loss": 1.5792,
      "step": 940
    },
    {
      "epoch": 0.10804240664460801,
      "grad_norm": 0.6327394843101501,
      "learning_rate": 0.0002676182893539581,
      "loss": 1.5817,
      "step": 950
    },
    {
      "epoch": 0.10917969513560388,
      "grad_norm": 0.6567193865776062,
      "learning_rate": 0.0002672770700636942,
      "loss": 1.5661,
      "step": 960
    },
    {
      "epoch": 0.11031698362659975,
      "grad_norm": 0.6162658929824829,
      "learning_rate": 0.0002669358507734304,
      "loss": 1.5798,
      "step": 970
    },
    {
      "epoch": 0.11145427211759563,
      "grad_norm": 0.6878103017807007,
      "learning_rate": 0.0002665946314831665,
      "loss": 1.5505,
      "step": 980
    },
    {
      "epoch": 0.1125915606085915,
      "grad_norm": 0.6983615756034851,
      "learning_rate": 0.0002662534121929026,
      "loss": 1.5738,
      "step": 990
    },
    {
      "epoch": 0.11372884909958737,
      "grad_norm": 0.5705863833427429,
      "learning_rate": 0.0002659121929026387,
      "loss": 1.5567,
      "step": 1000
    },
    {
      "epoch": 0.11486613759058326,
      "grad_norm": 0.6102845072746277,
      "learning_rate": 0.0002655709736123749,
      "loss": 1.5132,
      "step": 1010
    },
    {
      "epoch": 0.11600342608157913,
      "grad_norm": 0.705461859703064,
      "learning_rate": 0.000265229754322111,
      "loss": 1.5719,
      "step": 1020
    },
    {
      "epoch": 0.117140714572575,
      "grad_norm": 0.6320573687553406,
      "learning_rate": 0.00026488853503184713,
      "loss": 1.5196,
      "step": 1030
    },
    {
      "epoch": 0.11827800306357088,
      "grad_norm": 0.6824849843978882,
      "learning_rate": 0.00026454731574158323,
      "loss": 1.5766,
      "step": 1040
    },
    {
      "epoch": 0.11941529155456675,
      "grad_norm": 0.755312442779541,
      "learning_rate": 0.0002642060964513194,
      "loss": 1.5452,
      "step": 1050
    },
    {
      "epoch": 0.12055258004556262,
      "grad_norm": 0.6753798127174377,
      "learning_rate": 0.0002638648771610555,
      "loss": 1.5655,
      "step": 1060
    },
    {
      "epoch": 0.1216898685365585,
      "grad_norm": 0.6937294602394104,
      "learning_rate": 0.0002635236578707916,
      "loss": 1.5363,
      "step": 1070
    },
    {
      "epoch": 0.12282715702755437,
      "grad_norm": 0.6110159754753113,
      "learning_rate": 0.00026318243858052773,
      "loss": 1.5712,
      "step": 1080
    },
    {
      "epoch": 0.12396444551855024,
      "grad_norm": 0.6418405771255493,
      "learning_rate": 0.0002628412192902639,
      "loss": 1.5791,
      "step": 1090
    },
    {
      "epoch": 0.1251017340095461,
      "grad_norm": 0.675166130065918,
      "learning_rate": 0.0002625,
      "loss": 1.5738,
      "step": 1100
    },
    {
      "epoch": 0.12623902250054198,
      "grad_norm": 0.6631921529769897,
      "learning_rate": 0.0002621587807097361,
      "loss": 1.5436,
      "step": 1110
    },
    {
      "epoch": 0.12737631099153787,
      "grad_norm": 0.6793879866600037,
      "learning_rate": 0.00026181756141947224,
      "loss": 1.5605,
      "step": 1120
    },
    {
      "epoch": 0.12851359948253374,
      "grad_norm": 0.6306456327438354,
      "learning_rate": 0.00026147634212920833,
      "loss": 1.5585,
      "step": 1130
    },
    {
      "epoch": 0.1296508879735296,
      "grad_norm": 0.6813814043998718,
      "learning_rate": 0.0002611351228389445,
      "loss": 1.5219,
      "step": 1140
    },
    {
      "epoch": 0.13078817646452548,
      "grad_norm": 0.6904894709587097,
      "learning_rate": 0.0002607939035486806,
      "loss": 1.5473,
      "step": 1150
    },
    {
      "epoch": 0.13192546495552135,
      "grad_norm": 0.7250284552574158,
      "learning_rate": 0.0002604526842584167,
      "loss": 1.5317,
      "step": 1160
    },
    {
      "epoch": 0.13306275344651722,
      "grad_norm": 0.6390792727470398,
      "learning_rate": 0.00026011146496815284,
      "loss": 1.5586,
      "step": 1170
    },
    {
      "epoch": 0.13420004193751311,
      "grad_norm": 0.8373141288757324,
      "learning_rate": 0.000259770245677889,
      "loss": 1.5366,
      "step": 1180
    },
    {
      "epoch": 0.13533733042850898,
      "grad_norm": 0.6356652975082397,
      "learning_rate": 0.0002594290263876251,
      "loss": 1.5519,
      "step": 1190
    },
    {
      "epoch": 0.13647461891950485,
      "grad_norm": 0.6667302846908569,
      "learning_rate": 0.0002590878070973612,
      "loss": 1.5343,
      "step": 1200
    },
    {
      "epoch": 0.13761190741050072,
      "grad_norm": 0.6187795996665955,
      "learning_rate": 0.00025874658780709734,
      "loss": 1.5626,
      "step": 1210
    },
    {
      "epoch": 0.1387491959014966,
      "grad_norm": 0.6189295649528503,
      "learning_rate": 0.0002584053685168335,
      "loss": 1.5589,
      "step": 1220
    },
    {
      "epoch": 0.13988648439249246,
      "grad_norm": 0.6816307306289673,
      "learning_rate": 0.0002580641492265696,
      "loss": 1.5837,
      "step": 1230
    },
    {
      "epoch": 0.14102377288348836,
      "grad_norm": 0.6215036511421204,
      "learning_rate": 0.0002577229299363057,
      "loss": 1.5548,
      "step": 1240
    },
    {
      "epoch": 0.14216106137448423,
      "grad_norm": 0.70831698179245,
      "learning_rate": 0.00025738171064604184,
      "loss": 1.5156,
      "step": 1250
    },
    {
      "epoch": 0.1432983498654801,
      "grad_norm": 0.6162073016166687,
      "learning_rate": 0.00025704049135577794,
      "loss": 1.4933,
      "step": 1260
    },
    {
      "epoch": 0.14443563835647596,
      "grad_norm": 0.6074591279029846,
      "learning_rate": 0.0002566992720655141,
      "loss": 1.5245,
      "step": 1270
    },
    {
      "epoch": 0.14557292684747183,
      "grad_norm": 0.6639560461044312,
      "learning_rate": 0.0002563580527752502,
      "loss": 1.6114,
      "step": 1280
    },
    {
      "epoch": 0.1467102153384677,
      "grad_norm": 0.6419628858566284,
      "learning_rate": 0.00025601683348498635,
      "loss": 1.5929,
      "step": 1290
    },
    {
      "epoch": 0.1478475038294636,
      "grad_norm": 0.536817729473114,
      "learning_rate": 0.00025567561419472245,
      "loss": 1.5062,
      "step": 1300
    },
    {
      "epoch": 0.14898479232045947,
      "grad_norm": 0.6633228063583374,
      "learning_rate": 0.00025533439490445855,
      "loss": 1.5362,
      "step": 1310
    },
    {
      "epoch": 0.15012208081145534,
      "grad_norm": 0.6023537516593933,
      "learning_rate": 0.0002549931756141947,
      "loss": 1.5152,
      "step": 1320
    },
    {
      "epoch": 0.1512593693024512,
      "grad_norm": 0.7473198175430298,
      "learning_rate": 0.00025465195632393085,
      "loss": 1.554,
      "step": 1330
    },
    {
      "epoch": 0.15239665779344708,
      "grad_norm": 0.6882949471473694,
      "learning_rate": 0.00025431073703366695,
      "loss": 1.5763,
      "step": 1340
    },
    {
      "epoch": 0.15353394628444295,
      "grad_norm": 0.588489830493927,
      "learning_rate": 0.00025396951774340305,
      "loss": 1.5737,
      "step": 1350
    },
    {
      "epoch": 0.15467123477543884,
      "grad_norm": 0.62242192029953,
      "learning_rate": 0.0002536282984531392,
      "loss": 1.5614,
      "step": 1360
    },
    {
      "epoch": 0.1558085232664347,
      "grad_norm": 0.6203092932701111,
      "learning_rate": 0.00025328707916287535,
      "loss": 1.5685,
      "step": 1370
    },
    {
      "epoch": 0.15694581175743058,
      "grad_norm": 0.6150710582733154,
      "learning_rate": 0.00025294585987261145,
      "loss": 1.5272,
      "step": 1380
    },
    {
      "epoch": 0.15808310024842645,
      "grad_norm": 0.6439207792282104,
      "learning_rate": 0.00025260464058234755,
      "loss": 1.5025,
      "step": 1390
    },
    {
      "epoch": 0.15922038873942232,
      "grad_norm": 0.6639576554298401,
      "learning_rate": 0.0002522634212920837,
      "loss": 1.5528,
      "step": 1400
    },
    {
      "epoch": 0.1603576772304182,
      "grad_norm": 0.6327205896377563,
      "learning_rate": 0.0002519222020018198,
      "loss": 1.5471,
      "step": 1410
    },
    {
      "epoch": 0.16149496572141409,
      "grad_norm": 0.6286746263504028,
      "learning_rate": 0.00025158098271155596,
      "loss": 1.5447,
      "step": 1420
    },
    {
      "epoch": 0.16263225421240995,
      "grad_norm": 0.6274347901344299,
      "learning_rate": 0.00025123976342129206,
      "loss": 1.5411,
      "step": 1430
    },
    {
      "epoch": 0.16376954270340582,
      "grad_norm": 0.6895303130149841,
      "learning_rate": 0.00025089854413102815,
      "loss": 1.5218,
      "step": 1440
    },
    {
      "epoch": 0.1649068311944017,
      "grad_norm": 0.6234954595565796,
      "learning_rate": 0.0002505573248407643,
      "loss": 1.5553,
      "step": 1450
    },
    {
      "epoch": 0.16604411968539756,
      "grad_norm": 0.5616582632064819,
      "learning_rate": 0.00025021610555050046,
      "loss": 1.5473,
      "step": 1460
    },
    {
      "epoch": 0.16718140817639343,
      "grad_norm": 0.6149778962135315,
      "learning_rate": 0.00024987488626023656,
      "loss": 1.5352,
      "step": 1470
    },
    {
      "epoch": 0.16831869666738933,
      "grad_norm": 0.6321725845336914,
      "learning_rate": 0.00024953366696997266,
      "loss": 1.5427,
      "step": 1480
    },
    {
      "epoch": 0.1694559851583852,
      "grad_norm": 0.6508787870407104,
      "learning_rate": 0.0002491924476797088,
      "loss": 1.5574,
      "step": 1490
    },
    {
      "epoch": 0.17059327364938107,
      "grad_norm": 0.5885937213897705,
      "learning_rate": 0.0002488512283894449,
      "loss": 1.5943,
      "step": 1500
    },
    {
      "epoch": 0.17173056214037694,
      "grad_norm": 0.6268905401229858,
      "learning_rate": 0.00024851000909918106,
      "loss": 1.551,
      "step": 1510
    },
    {
      "epoch": 0.1728678506313728,
      "grad_norm": 0.7211498618125916,
      "learning_rate": 0.00024816878980891716,
      "loss": 1.523,
      "step": 1520
    },
    {
      "epoch": 0.17400513912236867,
      "grad_norm": 0.598910391330719,
      "learning_rate": 0.0002478275705186533,
      "loss": 1.5166,
      "step": 1530
    },
    {
      "epoch": 0.17514242761336457,
      "grad_norm": 0.6701720356941223,
      "learning_rate": 0.0002474863512283894,
      "loss": 1.567,
      "step": 1540
    },
    {
      "epoch": 0.17627971610436044,
      "grad_norm": 0.6704549789428711,
      "learning_rate": 0.00024714513193812557,
      "loss": 1.5585,
      "step": 1550
    },
    {
      "epoch": 0.1774170045953563,
      "grad_norm": 0.7658980488777161,
      "learning_rate": 0.00024680391264786166,
      "loss": 1.5535,
      "step": 1560
    },
    {
      "epoch": 0.17855429308635218,
      "grad_norm": 0.6173233389854431,
      "learning_rate": 0.0002464626933575978,
      "loss": 1.5489,
      "step": 1570
    },
    {
      "epoch": 0.17969158157734805,
      "grad_norm": 0.6807106733322144,
      "learning_rate": 0.0002461214740673339,
      "loss": 1.5615,
      "step": 1580
    },
    {
      "epoch": 0.18082887006834392,
      "grad_norm": 0.6702905893325806,
      "learning_rate": 0.00024578025477707,
      "loss": 1.5818,
      "step": 1590
    },
    {
      "epoch": 0.1819661585593398,
      "grad_norm": 0.6443264484405518,
      "learning_rate": 0.00024543903548680617,
      "loss": 1.5268,
      "step": 1600
    },
    {
      "epoch": 0.18310344705033568,
      "grad_norm": 0.669818103313446,
      "learning_rate": 0.0002450978161965423,
      "loss": 1.566,
      "step": 1610
    },
    {
      "epoch": 0.18424073554133155,
      "grad_norm": 0.6672519445419312,
      "learning_rate": 0.0002447565969062784,
      "loss": 1.5171,
      "step": 1620
    },
    {
      "epoch": 0.18537802403232742,
      "grad_norm": 0.7041763663291931,
      "learning_rate": 0.0002444153776160145,
      "loss": 1.5532,
      "step": 1630
    },
    {
      "epoch": 0.1865153125233233,
      "grad_norm": 0.6229740977287292,
      "learning_rate": 0.00024407415832575064,
      "loss": 1.5305,
      "step": 1640
    },
    {
      "epoch": 0.18765260101431916,
      "grad_norm": 0.6001971364021301,
      "learning_rate": 0.0002437329390354868,
      "loss": 1.4956,
      "step": 1650
    },
    {
      "epoch": 0.18878988950531506,
      "grad_norm": 0.6490011215209961,
      "learning_rate": 0.00024339171974522292,
      "loss": 1.552,
      "step": 1660
    },
    {
      "epoch": 0.18992717799631093,
      "grad_norm": 0.6324980854988098,
      "learning_rate": 0.00024305050045495902,
      "loss": 1.5191,
      "step": 1670
    },
    {
      "epoch": 0.1910644664873068,
      "grad_norm": 0.6755667924880981,
      "learning_rate": 0.00024270928116469515,
      "loss": 1.5539,
      "step": 1680
    },
    {
      "epoch": 0.19220175497830266,
      "grad_norm": 0.7075906991958618,
      "learning_rate": 0.0002423680618744313,
      "loss": 1.5102,
      "step": 1690
    },
    {
      "epoch": 0.19333904346929853,
      "grad_norm": 0.6543720960617065,
      "learning_rate": 0.00024202684258416743,
      "loss": 1.5364,
      "step": 1700
    },
    {
      "epoch": 0.1944763319602944,
      "grad_norm": 0.6262208819389343,
      "learning_rate": 0.00024168562329390352,
      "loss": 1.5347,
      "step": 1710
    },
    {
      "epoch": 0.1956136204512903,
      "grad_norm": 0.6257677674293518,
      "learning_rate": 0.00024134440400363965,
      "loss": 1.5983,
      "step": 1720
    },
    {
      "epoch": 0.19675090894228617,
      "grad_norm": 0.6305930614471436,
      "learning_rate": 0.00024100318471337578,
      "loss": 1.5452,
      "step": 1730
    },
    {
      "epoch": 0.19788819743328204,
      "grad_norm": 0.6661778092384338,
      "learning_rate": 0.0002406619654231119,
      "loss": 1.5776,
      "step": 1740
    },
    {
      "epoch": 0.1990254859242779,
      "grad_norm": 0.6110304594039917,
      "learning_rate": 0.00024032074613284803,
      "loss": 1.5252,
      "step": 1750
    },
    {
      "epoch": 0.20016277441527378,
      "grad_norm": 0.6787856817245483,
      "learning_rate": 0.00023997952684258415,
      "loss": 1.5286,
      "step": 1760
    },
    {
      "epoch": 0.20130006290626964,
      "grad_norm": 0.5605239272117615,
      "learning_rate": 0.00023963830755232025,
      "loss": 1.5183,
      "step": 1770
    },
    {
      "epoch": 0.20243735139726554,
      "grad_norm": 0.5989757776260376,
      "learning_rate": 0.00023929708826205638,
      "loss": 1.5296,
      "step": 1780
    },
    {
      "epoch": 0.2035746398882614,
      "grad_norm": 0.6495345830917358,
      "learning_rate": 0.00023895586897179253,
      "loss": 1.4492,
      "step": 1790
    },
    {
      "epoch": 0.20471192837925728,
      "grad_norm": 0.6029282212257385,
      "learning_rate": 0.00023861464968152866,
      "loss": 1.5064,
      "step": 1800
    },
    {
      "epoch": 0.20584921687025315,
      "grad_norm": 0.6092548370361328,
      "learning_rate": 0.00023827343039126476,
      "loss": 1.5524,
      "step": 1810
    },
    {
      "epoch": 0.20698650536124902,
      "grad_norm": 0.625641942024231,
      "learning_rate": 0.00023793221110100088,
      "loss": 1.4947,
      "step": 1820
    },
    {
      "epoch": 0.2081237938522449,
      "grad_norm": 0.6297009587287903,
      "learning_rate": 0.000237590991810737,
      "loss": 1.5092,
      "step": 1830
    },
    {
      "epoch": 0.20926108234324078,
      "grad_norm": 0.6829903721809387,
      "learning_rate": 0.00023724977252047316,
      "loss": 1.5573,
      "step": 1840
    },
    {
      "epoch": 0.21039837083423665,
      "grad_norm": 0.6741893887519836,
      "learning_rate": 0.00023690855323020926,
      "loss": 1.5974,
      "step": 1850
    },
    {
      "epoch": 0.21153565932523252,
      "grad_norm": 0.6610151529312134,
      "learning_rate": 0.00023656733393994538,
      "loss": 1.586,
      "step": 1860
    },
    {
      "epoch": 0.2126729478162284,
      "grad_norm": 0.6524074077606201,
      "learning_rate": 0.0002362261146496815,
      "loss": 1.5385,
      "step": 1870
    },
    {
      "epoch": 0.21381023630722426,
      "grad_norm": 0.6142225861549377,
      "learning_rate": 0.00023588489535941764,
      "loss": 1.5704,
      "step": 1880
    },
    {
      "epoch": 0.21494752479822013,
      "grad_norm": 0.5637310743331909,
      "learning_rate": 0.00023554367606915376,
      "loss": 1.5431,
      "step": 1890
    },
    {
      "epoch": 0.21608481328921603,
      "grad_norm": 0.586384654045105,
      "learning_rate": 0.0002352024567788899,
      "loss": 1.5211,
      "step": 1900
    },
    {
      "epoch": 0.2172221017802119,
      "grad_norm": 0.688991129398346,
      "learning_rate": 0.000234861237488626,
      "loss": 1.5077,
      "step": 1910
    },
    {
      "epoch": 0.21835939027120777,
      "grad_norm": 0.6868801116943359,
      "learning_rate": 0.0002345200181983621,
      "loss": 1.5611,
      "step": 1920
    },
    {
      "epoch": 0.21949667876220363,
      "grad_norm": 0.621074914932251,
      "learning_rate": 0.00023417879890809827,
      "loss": 1.5397,
      "step": 1930
    },
    {
      "epoch": 0.2206339672531995,
      "grad_norm": 0.6524959802627563,
      "learning_rate": 0.0002338375796178344,
      "loss": 1.5179,
      "step": 1940
    },
    {
      "epoch": 0.22177125574419537,
      "grad_norm": 0.6952046751976013,
      "learning_rate": 0.0002334963603275705,
      "loss": 1.5331,
      "step": 1950
    },
    {
      "epoch": 0.22290854423519127,
      "grad_norm": 0.6889544129371643,
      "learning_rate": 0.00023315514103730662,
      "loss": 1.5382,
      "step": 1960
    },
    {
      "epoch": 0.22404583272618714,
      "grad_norm": 0.6279842853546143,
      "learning_rate": 0.00023281392174704274,
      "loss": 1.6096,
      "step": 1970
    },
    {
      "epoch": 0.225183121217183,
      "grad_norm": 0.5980408191680908,
      "learning_rate": 0.00023247270245677887,
      "loss": 1.521,
      "step": 1980
    },
    {
      "epoch": 0.22632040970817888,
      "grad_norm": 0.6621853113174438,
      "learning_rate": 0.000232131483166515,
      "loss": 1.5181,
      "step": 1990
    },
    {
      "epoch": 0.22745769819917475,
      "grad_norm": 0.6068701148033142,
      "learning_rate": 0.00023179026387625112,
      "loss": 1.5353,
      "step": 2000
    },
    {
      "epoch": 0.22859498669017062,
      "grad_norm": 0.712518036365509,
      "learning_rate": 0.00023144904458598722,
      "loss": 1.4947,
      "step": 2010
    },
    {
      "epoch": 0.2297322751811665,
      "grad_norm": 0.6753366589546204,
      "learning_rate": 0.00023110782529572337,
      "loss": 1.5025,
      "step": 2020
    },
    {
      "epoch": 0.23086956367216238,
      "grad_norm": 0.5562598705291748,
      "learning_rate": 0.0002307666060054595,
      "loss": 1.4864,
      "step": 2030
    },
    {
      "epoch": 0.23200685216315825,
      "grad_norm": 0.5658987164497375,
      "learning_rate": 0.00023042538671519562,
      "loss": 1.5872,
      "step": 2040
    },
    {
      "epoch": 0.23314414065415412,
      "grad_norm": 0.6152125000953674,
      "learning_rate": 0.00023008416742493172,
      "loss": 1.5338,
      "step": 2050
    },
    {
      "epoch": 0.23428142914515,
      "grad_norm": 0.6599099040031433,
      "learning_rate": 0.00022974294813466785,
      "loss": 1.5392,
      "step": 2060
    },
    {
      "epoch": 0.23541871763614586,
      "grad_norm": 0.6832498908042908,
      "learning_rate": 0.000229401728844404,
      "loss": 1.5869,
      "step": 2070
    },
    {
      "epoch": 0.23655600612714175,
      "grad_norm": 0.6472939848899841,
      "learning_rate": 0.00022906050955414013,
      "loss": 1.5253,
      "step": 2080
    },
    {
      "epoch": 0.23769329461813762,
      "grad_norm": 0.7334678173065186,
      "learning_rate": 0.00022871929026387622,
      "loss": 1.5178,
      "step": 2090
    },
    {
      "epoch": 0.2388305831091335,
      "grad_norm": 0.6619014739990234,
      "learning_rate": 0.00022837807097361235,
      "loss": 1.5128,
      "step": 2100
    },
    {
      "epoch": 0.23996787160012936,
      "grad_norm": 0.6065704822540283,
      "learning_rate": 0.00022803685168334848,
      "loss": 1.5259,
      "step": 2110
    },
    {
      "epoch": 0.24110516009112523,
      "grad_norm": 0.5554165840148926,
      "learning_rate": 0.0002276956323930846,
      "loss": 1.5059,
      "step": 2120
    },
    {
      "epoch": 0.2422424485821211,
      "grad_norm": 0.7216259837150574,
      "learning_rate": 0.00022735441310282073,
      "loss": 1.5079,
      "step": 2130
    },
    {
      "epoch": 0.243379737073117,
      "grad_norm": 0.6608061194419861,
      "learning_rate": 0.00022701319381255685,
      "loss": 1.5384,
      "step": 2140
    },
    {
      "epoch": 0.24451702556411287,
      "grad_norm": 0.6749731302261353,
      "learning_rate": 0.00022667197452229295,
      "loss": 1.566,
      "step": 2150
    },
    {
      "epoch": 0.24565431405510874,
      "grad_norm": 0.6415789723396301,
      "learning_rate": 0.00022633075523202908,
      "loss": 1.5137,
      "step": 2160
    },
    {
      "epoch": 0.2467916025461046,
      "grad_norm": 0.6737242937088013,
      "learning_rate": 0.00022598953594176523,
      "loss": 1.5063,
      "step": 2170
    },
    {
      "epoch": 0.24792889103710047,
      "grad_norm": 0.6513852477073669,
      "learning_rate": 0.00022564831665150136,
      "loss": 1.5145,
      "step": 2180
    },
    {
      "epoch": 0.24906617952809634,
      "grad_norm": 0.5760984420776367,
      "learning_rate": 0.00022530709736123746,
      "loss": 1.5721,
      "step": 2190
    },
    {
      "epoch": 0.2502034680190922,
      "grad_norm": 0.5996518135070801,
      "learning_rate": 0.00022496587807097358,
      "loss": 1.56,
      "step": 2200
    },
    {
      "epoch": 0.2513407565100881,
      "grad_norm": 0.6848626732826233,
      "learning_rate": 0.00022462465878070973,
      "loss": 1.5571,
      "step": 2210
    },
    {
      "epoch": 0.25247804500108395,
      "grad_norm": 0.6564136743545532,
      "learning_rate": 0.00022428343949044586,
      "loss": 1.5703,
      "step": 2220
    },
    {
      "epoch": 0.2536153334920799,
      "grad_norm": 0.6348637342453003,
      "learning_rate": 0.00022394222020018196,
      "loss": 1.5227,
      "step": 2230
    },
    {
      "epoch": 0.25475262198307574,
      "grad_norm": 0.621527373790741,
      "learning_rate": 0.00022360100090991809,
      "loss": 1.4993,
      "step": 2240
    },
    {
      "epoch": 0.2558899104740716,
      "grad_norm": 0.6033338308334351,
      "learning_rate": 0.0002232597816196542,
      "loss": 1.5754,
      "step": 2250
    },
    {
      "epoch": 0.2570271989650675,
      "grad_norm": 0.6212242245674133,
      "learning_rate": 0.00022291856232939034,
      "loss": 1.5469,
      "step": 2260
    },
    {
      "epoch": 0.25816448745606335,
      "grad_norm": 0.7149098515510559,
      "learning_rate": 0.00022257734303912646,
      "loss": 1.5704,
      "step": 2270
    },
    {
      "epoch": 0.2593017759470592,
      "grad_norm": 0.6928788423538208,
      "learning_rate": 0.0002222361237488626,
      "loss": 1.5186,
      "step": 2280
    },
    {
      "epoch": 0.2604390644380551,
      "grad_norm": 0.6543766260147095,
      "learning_rate": 0.0002218949044585987,
      "loss": 1.5222,
      "step": 2290
    },
    {
      "epoch": 0.26157635292905096,
      "grad_norm": 0.7500181794166565,
      "learning_rate": 0.0002215536851683348,
      "loss": 1.5726,
      "step": 2300
    },
    {
      "epoch": 0.26271364142004683,
      "grad_norm": 0.6137121915817261,
      "learning_rate": 0.00022121246587807097,
      "loss": 1.5538,
      "step": 2310
    },
    {
      "epoch": 0.2638509299110427,
      "grad_norm": 0.5771912932395935,
      "learning_rate": 0.0002208712465878071,
      "loss": 1.5118,
      "step": 2320
    },
    {
      "epoch": 0.26498821840203857,
      "grad_norm": 0.6394271850585938,
      "learning_rate": 0.0002205300272975432,
      "loss": 1.5564,
      "step": 2330
    },
    {
      "epoch": 0.26612550689303444,
      "grad_norm": 0.5852206349372864,
      "learning_rate": 0.00022018880800727932,
      "loss": 1.5845,
      "step": 2340
    },
    {
      "epoch": 0.26726279538403036,
      "grad_norm": 0.5824687480926514,
      "learning_rate": 0.00021984758871701547,
      "loss": 1.5259,
      "step": 2350
    },
    {
      "epoch": 0.26840008387502623,
      "grad_norm": 0.6136112213134766,
      "learning_rate": 0.0002195063694267516,
      "loss": 1.5485,
      "step": 2360
    },
    {
      "epoch": 0.2695373723660221,
      "grad_norm": 0.5833795666694641,
      "learning_rate": 0.0002191651501364877,
      "loss": 1.5629,
      "step": 2370
    },
    {
      "epoch": 0.27067466085701797,
      "grad_norm": 0.6322593092918396,
      "learning_rate": 0.00021882393084622382,
      "loss": 1.5481,
      "step": 2380
    },
    {
      "epoch": 0.27181194934801384,
      "grad_norm": 0.6108348965644836,
      "learning_rate": 0.00021848271155595995,
      "loss": 1.5433,
      "step": 2390
    },
    {
      "epoch": 0.2729492378390097,
      "grad_norm": 0.6565352082252502,
      "learning_rate": 0.00021814149226569607,
      "loss": 1.5733,
      "step": 2400
    },
    {
      "epoch": 0.2740865263300056,
      "grad_norm": 0.6738814115524292,
      "learning_rate": 0.0002178002729754322,
      "loss": 1.5154,
      "step": 2410
    },
    {
      "epoch": 0.27522381482100144,
      "grad_norm": 0.6178128123283386,
      "learning_rate": 0.00021745905368516832,
      "loss": 1.5861,
      "step": 2420
    },
    {
      "epoch": 0.2763611033119973,
      "grad_norm": 0.61461341381073,
      "learning_rate": 0.00021711783439490442,
      "loss": 1.5013,
      "step": 2430
    },
    {
      "epoch": 0.2774983918029932,
      "grad_norm": 0.6140667796134949,
      "learning_rate": 0.00021677661510464055,
      "loss": 1.5483,
      "step": 2440
    },
    {
      "epoch": 0.27863568029398905,
      "grad_norm": 0.6481608748435974,
      "learning_rate": 0.0002164353958143767,
      "loss": 1.5281,
      "step": 2450
    },
    {
      "epoch": 0.2797729687849849,
      "grad_norm": 0.6958401203155518,
      "learning_rate": 0.00021609417652411283,
      "loss": 1.518,
      "step": 2460
    },
    {
      "epoch": 0.28091025727598085,
      "grad_norm": 0.6229265928268433,
      "learning_rate": 0.00021575295723384893,
      "loss": 1.5093,
      "step": 2470
    },
    {
      "epoch": 0.2820475457669767,
      "grad_norm": 0.6721715331077576,
      "learning_rate": 0.00021541173794358505,
      "loss": 1.5061,
      "step": 2480
    },
    {
      "epoch": 0.2831848342579726,
      "grad_norm": 0.6671632528305054,
      "learning_rate": 0.00021507051865332118,
      "loss": 1.5817,
      "step": 2490
    },
    {
      "epoch": 0.28432212274896845,
      "grad_norm": 0.6497926115989685,
      "learning_rate": 0.00021472929936305733,
      "loss": 1.4932,
      "step": 2500
    },
    {
      "epoch": 0.2854594112399643,
      "grad_norm": 0.6606916189193726,
      "learning_rate": 0.00021438808007279343,
      "loss": 1.5968,
      "step": 2510
    },
    {
      "epoch": 0.2865966997309602,
      "grad_norm": 0.5690532326698303,
      "learning_rate": 0.00021404686078252955,
      "loss": 1.5314,
      "step": 2520
    },
    {
      "epoch": 0.28773398822195606,
      "grad_norm": 0.5967826247215271,
      "learning_rate": 0.00021370564149226565,
      "loss": 1.5074,
      "step": 2530
    },
    {
      "epoch": 0.28887127671295193,
      "grad_norm": 0.5985894203186035,
      "learning_rate": 0.0002133644222020018,
      "loss": 1.5437,
      "step": 2540
    },
    {
      "epoch": 0.2900085652039478,
      "grad_norm": 0.61287921667099,
      "learning_rate": 0.00021302320291173793,
      "loss": 1.5143,
      "step": 2550
    },
    {
      "epoch": 0.29114585369494367,
      "grad_norm": 0.587300181388855,
      "learning_rate": 0.00021268198362147406,
      "loss": 1.4742,
      "step": 2560
    },
    {
      "epoch": 0.29228314218593954,
      "grad_norm": 0.6438612937927246,
      "learning_rate": 0.00021234076433121016,
      "loss": 1.5196,
      "step": 2570
    },
    {
      "epoch": 0.2934204306769354,
      "grad_norm": 0.6298653483390808,
      "learning_rate": 0.00021199954504094628,
      "loss": 1.4907,
      "step": 2580
    },
    {
      "epoch": 0.29455771916793133,
      "grad_norm": 0.6793068647384644,
      "learning_rate": 0.00021165832575068244,
      "loss": 1.4651,
      "step": 2590
    },
    {
      "epoch": 0.2956950076589272,
      "grad_norm": 0.639817476272583,
      "learning_rate": 0.00021131710646041856,
      "loss": 1.5336,
      "step": 2600
    },
    {
      "epoch": 0.29683229614992307,
      "grad_norm": 0.6361089944839478,
      "learning_rate": 0.00021097588717015466,
      "loss": 1.523,
      "step": 2610
    },
    {
      "epoch": 0.29796958464091894,
      "grad_norm": 0.5715321898460388,
      "learning_rate": 0.00021063466787989079,
      "loss": 1.5283,
      "step": 2620
    },
    {
      "epoch": 0.2991068731319148,
      "grad_norm": 0.6560509204864502,
      "learning_rate": 0.0002102934485896269,
      "loss": 1.5195,
      "step": 2630
    },
    {
      "epoch": 0.3002441616229107,
      "grad_norm": 0.6298555135726929,
      "learning_rate": 0.00020995222929936304,
      "loss": 1.5336,
      "step": 2640
    },
    {
      "epoch": 0.30138145011390655,
      "grad_norm": 0.5958001613616943,
      "learning_rate": 0.00020961101000909916,
      "loss": 1.5383,
      "step": 2650
    },
    {
      "epoch": 0.3025187386049024,
      "grad_norm": 0.6267204880714417,
      "learning_rate": 0.0002092697907188353,
      "loss": 1.555,
      "step": 2660
    },
    {
      "epoch": 0.3036560270958983,
      "grad_norm": 0.5715139508247375,
      "learning_rate": 0.0002089285714285714,
      "loss": 1.4952,
      "step": 2670
    },
    {
      "epoch": 0.30479331558689415,
      "grad_norm": 0.6228523850440979,
      "learning_rate": 0.00020858735213830751,
      "loss": 1.5179,
      "step": 2680
    },
    {
      "epoch": 0.30593060407789,
      "grad_norm": 0.6425083875656128,
      "learning_rate": 0.00020824613284804367,
      "loss": 1.5369,
      "step": 2690
    },
    {
      "epoch": 0.3070678925688859,
      "grad_norm": 0.6893554329872131,
      "learning_rate": 0.0002079049135577798,
      "loss": 1.4886,
      "step": 2700
    },
    {
      "epoch": 0.3082051810598818,
      "grad_norm": 0.5947287082672119,
      "learning_rate": 0.0002075636942675159,
      "loss": 1.5623,
      "step": 2710
    },
    {
      "epoch": 0.3093424695508777,
      "grad_norm": 0.729301393032074,
      "learning_rate": 0.00020722247497725202,
      "loss": 1.5406,
      "step": 2720
    },
    {
      "epoch": 0.31047975804187355,
      "grad_norm": 0.6717922687530518,
      "learning_rate": 0.00020688125568698817,
      "loss": 1.5176,
      "step": 2730
    },
    {
      "epoch": 0.3116170465328694,
      "grad_norm": 0.7117012739181519,
      "learning_rate": 0.0002065400363967243,
      "loss": 1.501,
      "step": 2740
    },
    {
      "epoch": 0.3127543350238653,
      "grad_norm": 0.6376917362213135,
      "learning_rate": 0.0002061988171064604,
      "loss": 1.5196,
      "step": 2750
    },
    {
      "epoch": 0.31389162351486116,
      "grad_norm": 0.6548277735710144,
      "learning_rate": 0.00020585759781619652,
      "loss": 1.5013,
      "step": 2760
    },
    {
      "epoch": 0.31502891200585703,
      "grad_norm": 0.5311452150344849,
      "learning_rate": 0.00020551637852593265,
      "loss": 1.5544,
      "step": 2770
    },
    {
      "epoch": 0.3161662004968529,
      "grad_norm": 0.602730393409729,
      "learning_rate": 0.00020517515923566877,
      "loss": 1.4929,
      "step": 2780
    },
    {
      "epoch": 0.31730348898784877,
      "grad_norm": 0.5892813801765442,
      "learning_rate": 0.0002048339399454049,
      "loss": 1.5264,
      "step": 2790
    },
    {
      "epoch": 0.31844077747884464,
      "grad_norm": 0.6477335095405579,
      "learning_rate": 0.00020449272065514102,
      "loss": 1.4959,
      "step": 2800
    },
    {
      "epoch": 0.3195780659698405,
      "grad_norm": 0.6459503173828125,
      "learning_rate": 0.00020415150136487712,
      "loss": 1.5455,
      "step": 2810
    },
    {
      "epoch": 0.3207153544608364,
      "grad_norm": 0.6223435997962952,
      "learning_rate": 0.00020381028207461325,
      "loss": 1.5257,
      "step": 2820
    },
    {
      "epoch": 0.3218526429518323,
      "grad_norm": 0.6376974582672119,
      "learning_rate": 0.0002034690627843494,
      "loss": 1.523,
      "step": 2830
    },
    {
      "epoch": 0.32298993144282817,
      "grad_norm": 0.6339737176895142,
      "learning_rate": 0.00020312784349408553,
      "loss": 1.542,
      "step": 2840
    },
    {
      "epoch": 0.32412721993382404,
      "grad_norm": 0.6567575931549072,
      "learning_rate": 0.00020278662420382163,
      "loss": 1.528,
      "step": 2850
    },
    {
      "epoch": 0.3252645084248199,
      "grad_norm": 0.5889131426811218,
      "learning_rate": 0.00020244540491355775,
      "loss": 1.5365,
      "step": 2860
    },
    {
      "epoch": 0.3264017969158158,
      "grad_norm": 0.7380403280258179,
      "learning_rate": 0.0002021041856232939,
      "loss": 1.5241,
      "step": 2870
    },
    {
      "epoch": 0.32753908540681165,
      "grad_norm": 0.5611009001731873,
      "learning_rate": 0.00020176296633303003,
      "loss": 1.5713,
      "step": 2880
    },
    {
      "epoch": 0.3286763738978075,
      "grad_norm": 0.613129198551178,
      "learning_rate": 0.00020142174704276613,
      "loss": 1.5002,
      "step": 2890
    },
    {
      "epoch": 0.3298136623888034,
      "grad_norm": 0.5746732354164124,
      "learning_rate": 0.00020108052775250226,
      "loss": 1.4935,
      "step": 2900
    },
    {
      "epoch": 0.33095095087979925,
      "grad_norm": 0.6157907843589783,
      "learning_rate": 0.00020073930846223838,
      "loss": 1.489,
      "step": 2910
    },
    {
      "epoch": 0.3320882393707951,
      "grad_norm": 0.7203415632247925,
      "learning_rate": 0.0002003980891719745,
      "loss": 1.5395,
      "step": 2920
    },
    {
      "epoch": 0.333225527861791,
      "grad_norm": 0.5917277336120605,
      "learning_rate": 0.00020005686988171063,
      "loss": 1.5584,
      "step": 2930
    },
    {
      "epoch": 0.33436281635278686,
      "grad_norm": 0.6032777428627014,
      "learning_rate": 0.00019971565059144676,
      "loss": 1.5531,
      "step": 2940
    },
    {
      "epoch": 0.3355001048437828,
      "grad_norm": 0.658454954624176,
      "learning_rate": 0.00019937443130118286,
      "loss": 1.5093,
      "step": 2950
    },
    {
      "epoch": 0.33663739333477866,
      "grad_norm": 0.647367000579834,
      "learning_rate": 0.00019903321201091898,
      "loss": 1.5101,
      "step": 2960
    },
    {
      "epoch": 0.3377746818257745,
      "grad_norm": 0.5800563097000122,
      "learning_rate": 0.00019869199272065514,
      "loss": 1.4823,
      "step": 2970
    },
    {
      "epoch": 0.3389119703167704,
      "grad_norm": 0.574505627155304,
      "learning_rate": 0.00019835077343039126,
      "loss": 1.5086,
      "step": 2980
    },
    {
      "epoch": 0.34004925880776626,
      "grad_norm": 0.6747032999992371,
      "learning_rate": 0.00019800955414012736,
      "loss": 1.513,
      "step": 2990
    },
    {
      "epoch": 0.34118654729876213,
      "grad_norm": 0.5967592000961304,
      "learning_rate": 0.0001976683348498635,
      "loss": 1.5541,
      "step": 3000
    },
    {
      "epoch": 0.342323835789758,
      "grad_norm": 0.6265314221382141,
      "learning_rate": 0.0001973271155595996,
      "loss": 1.5547,
      "step": 3010
    },
    {
      "epoch": 0.34346112428075387,
      "grad_norm": 0.6375995874404907,
      "learning_rate": 0.00019698589626933577,
      "loss": 1.5233,
      "step": 3020
    },
    {
      "epoch": 0.34459841277174974,
      "grad_norm": 0.7113350033760071,
      "learning_rate": 0.00019664467697907186,
      "loss": 1.5392,
      "step": 3030
    },
    {
      "epoch": 0.3457357012627456,
      "grad_norm": 0.6129426956176758,
      "learning_rate": 0.000196303457688808,
      "loss": 1.5209,
      "step": 3040
    },
    {
      "epoch": 0.3468729897537415,
      "grad_norm": 0.7004408240318298,
      "learning_rate": 0.00019596223839854412,
      "loss": 1.52,
      "step": 3050
    },
    {
      "epoch": 0.34801027824473735,
      "grad_norm": 0.6532754898071289,
      "learning_rate": 0.00019562101910828024,
      "loss": 1.4982,
      "step": 3060
    },
    {
      "epoch": 0.3491475667357333,
      "grad_norm": 0.6342798471450806,
      "learning_rate": 0.00019527979981801637,
      "loss": 1.5568,
      "step": 3070
    },
    {
      "epoch": 0.35028485522672914,
      "grad_norm": 0.6423735022544861,
      "learning_rate": 0.0001949385805277525,
      "loss": 1.5543,
      "step": 3080
    },
    {
      "epoch": 0.351422143717725,
      "grad_norm": 0.64792799949646,
      "learning_rate": 0.0001945973612374886,
      "loss": 1.5199,
      "step": 3090
    },
    {
      "epoch": 0.3525594322087209,
      "grad_norm": 0.6308525800704956,
      "learning_rate": 0.00019425614194722472,
      "loss": 1.5367,
      "step": 3100
    },
    {
      "epoch": 0.35369672069971675,
      "grad_norm": 0.6655028462409973,
      "learning_rate": 0.00019391492265696087,
      "loss": 1.5418,
      "step": 3110
    },
    {
      "epoch": 0.3548340091907126,
      "grad_norm": 0.5892906188964844,
      "learning_rate": 0.000193573703366697,
      "loss": 1.502,
      "step": 3120
    },
    {
      "epoch": 0.3559712976817085,
      "grad_norm": 0.6341612935066223,
      "learning_rate": 0.0001932324840764331,
      "loss": 1.5361,
      "step": 3130
    },
    {
      "epoch": 0.35710858617270436,
      "grad_norm": 0.6311206221580505,
      "learning_rate": 0.00019289126478616922,
      "loss": 1.5191,
      "step": 3140
    },
    {
      "epoch": 0.3582458746637002,
      "grad_norm": 0.6179074048995972,
      "learning_rate": 0.00019255004549590535,
      "loss": 1.5548,
      "step": 3150
    },
    {
      "epoch": 0.3593831631546961,
      "grad_norm": 0.5901206731796265,
      "learning_rate": 0.00019220882620564147,
      "loss": 1.5004,
      "step": 3160
    },
    {
      "epoch": 0.36052045164569196,
      "grad_norm": 0.6448554396629333,
      "learning_rate": 0.0001918676069153776,
      "loss": 1.5407,
      "step": 3170
    },
    {
      "epoch": 0.36165774013668783,
      "grad_norm": 0.5980580449104309,
      "learning_rate": 0.00019152638762511372,
      "loss": 1.5631,
      "step": 3180
    },
    {
      "epoch": 0.36279502862768376,
      "grad_norm": 0.6438434720039368,
      "learning_rate": 0.00019118516833484982,
      "loss": 1.5644,
      "step": 3190
    },
    {
      "epoch": 0.3639323171186796,
      "grad_norm": 0.6724339127540588,
      "learning_rate": 0.00019084394904458598,
      "loss": 1.5806,
      "step": 3200
    },
    {
      "epoch": 0.3650696056096755,
      "grad_norm": 0.654369056224823,
      "learning_rate": 0.0001905027297543221,
      "loss": 1.5807,
      "step": 3210
    },
    {
      "epoch": 0.36620689410067137,
      "grad_norm": 0.6658820509910583,
      "learning_rate": 0.00019016151046405823,
      "loss": 1.5548,
      "step": 3220
    },
    {
      "epoch": 0.36734418259166723,
      "grad_norm": 0.6473528742790222,
      "learning_rate": 0.00018982029117379433,
      "loss": 1.54,
      "step": 3230
    },
    {
      "epoch": 0.3684814710826631,
      "grad_norm": 0.673641562461853,
      "learning_rate": 0.00018947907188353045,
      "loss": 1.5484,
      "step": 3240
    },
    {
      "epoch": 0.369618759573659,
      "grad_norm": 0.5571417808532715,
      "learning_rate": 0.0001891378525932666,
      "loss": 1.509,
      "step": 3250
    },
    {
      "epoch": 0.37075604806465484,
      "grad_norm": 0.6930130124092102,
      "learning_rate": 0.00018879663330300273,
      "loss": 1.5554,
      "step": 3260
    },
    {
      "epoch": 0.3718933365556507,
      "grad_norm": 0.6022243499755859,
      "learning_rate": 0.00018845541401273883,
      "loss": 1.5035,
      "step": 3270
    },
    {
      "epoch": 0.3730306250466466,
      "grad_norm": 0.592013418674469,
      "learning_rate": 0.00018811419472247496,
      "loss": 1.4929,
      "step": 3280
    },
    {
      "epoch": 0.37416791353764245,
      "grad_norm": 0.608238697052002,
      "learning_rate": 0.00018777297543221108,
      "loss": 1.4801,
      "step": 3290
    },
    {
      "epoch": 0.3753052020286383,
      "grad_norm": 0.6242277026176453,
      "learning_rate": 0.0001874317561419472,
      "loss": 1.4865,
      "step": 3300
    },
    {
      "epoch": 0.37644249051963424,
      "grad_norm": 0.6620084047317505,
      "learning_rate": 0.00018709053685168333,
      "loss": 1.5276,
      "step": 3310
    },
    {
      "epoch": 0.3775797790106301,
      "grad_norm": 0.7272941470146179,
      "learning_rate": 0.00018674931756141946,
      "loss": 1.5243,
      "step": 3320
    },
    {
      "epoch": 0.378717067501626,
      "grad_norm": 0.6748214960098267,
      "learning_rate": 0.00018640809827115556,
      "loss": 1.5665,
      "step": 3330
    },
    {
      "epoch": 0.37985435599262185,
      "grad_norm": 0.60694420337677,
      "learning_rate": 0.00018606687898089168,
      "loss": 1.535,
      "step": 3340
    },
    {
      "epoch": 0.3809916444836177,
      "grad_norm": 0.6407953500747681,
      "learning_rate": 0.00018572565969062784,
      "loss": 1.562,
      "step": 3350
    },
    {
      "epoch": 0.3821289329746136,
      "grad_norm": 0.6231087446212769,
      "learning_rate": 0.00018538444040036396,
      "loss": 1.5412,
      "step": 3360
    },
    {
      "epoch": 0.38326622146560946,
      "grad_norm": 0.6454198956489563,
      "learning_rate": 0.00018504322111010006,
      "loss": 1.5423,
      "step": 3370
    },
    {
      "epoch": 0.3844035099566053,
      "grad_norm": 0.6392416954040527,
      "learning_rate": 0.0001847020018198362,
      "loss": 1.4929,
      "step": 3380
    },
    {
      "epoch": 0.3855407984476012,
      "grad_norm": 0.6394402384757996,
      "learning_rate": 0.00018436078252957234,
      "loss": 1.5143,
      "step": 3390
    },
    {
      "epoch": 0.38667808693859707,
      "grad_norm": 0.6328190565109253,
      "learning_rate": 0.00018401956323930847,
      "loss": 1.5534,
      "step": 3400
    },
    {
      "epoch": 0.38781537542959293,
      "grad_norm": 0.5984847545623779,
      "learning_rate": 0.00018367834394904456,
      "loss": 1.4821,
      "step": 3410
    },
    {
      "epoch": 0.3889526639205888,
      "grad_norm": 0.578118085861206,
      "learning_rate": 0.0001833371246587807,
      "loss": 1.5232,
      "step": 3420
    },
    {
      "epoch": 0.39008995241158473,
      "grad_norm": 0.5971584320068359,
      "learning_rate": 0.00018299590536851682,
      "loss": 1.5314,
      "step": 3430
    },
    {
      "epoch": 0.3912272409025806,
      "grad_norm": 0.5935363173484802,
      "learning_rate": 0.00018265468607825294,
      "loss": 1.5368,
      "step": 3440
    },
    {
      "epoch": 0.39236452939357647,
      "grad_norm": 0.6439829468727112,
      "learning_rate": 0.00018231346678798907,
      "loss": 1.5045,
      "step": 3450
    },
    {
      "epoch": 0.39350181788457234,
      "grad_norm": 0.6293803453445435,
      "learning_rate": 0.0001819722474977252,
      "loss": 1.5062,
      "step": 3460
    },
    {
      "epoch": 0.3946391063755682,
      "grad_norm": 0.658684253692627,
      "learning_rate": 0.0001816310282074613,
      "loss": 1.5147,
      "step": 3470
    },
    {
      "epoch": 0.3957763948665641,
      "grad_norm": 0.6563486456871033,
      "learning_rate": 0.00018128980891719742,
      "loss": 1.5298,
      "step": 3480
    },
    {
      "epoch": 0.39691368335755994,
      "grad_norm": 0.748071551322937,
      "learning_rate": 0.00018094858962693357,
      "loss": 1.5142,
      "step": 3490
    },
    {
      "epoch": 0.3980509718485558,
      "grad_norm": 0.6532089114189148,
      "learning_rate": 0.0001806073703366697,
      "loss": 1.5509,
      "step": 3500
    },
    {
      "epoch": 0.3991882603395517,
      "grad_norm": 0.6642560362815857,
      "learning_rate": 0.0001802661510464058,
      "loss": 1.523,
      "step": 3510
    },
    {
      "epoch": 0.40032554883054755,
      "grad_norm": 0.6411679983139038,
      "learning_rate": 0.00017992493175614192,
      "loss": 1.5606,
      "step": 3520
    },
    {
      "epoch": 0.4014628373215434,
      "grad_norm": 0.6261773109436035,
      "learning_rate": 0.00017958371246587807,
      "loss": 1.5305,
      "step": 3530
    },
    {
      "epoch": 0.4026001258125393,
      "grad_norm": 0.6921061873435974,
      "learning_rate": 0.0001792424931756142,
      "loss": 1.5381,
      "step": 3540
    },
    {
      "epoch": 0.4037374143035352,
      "grad_norm": 0.596473217010498,
      "learning_rate": 0.0001789012738853503,
      "loss": 1.5007,
      "step": 3550
    },
    {
      "epoch": 0.4048747027945311,
      "grad_norm": 0.6039638519287109,
      "learning_rate": 0.00017856005459508642,
      "loss": 1.5527,
      "step": 3560
    },
    {
      "epoch": 0.40601199128552695,
      "grad_norm": 0.5924370288848877,
      "learning_rate": 0.00017821883530482255,
      "loss": 1.5393,
      "step": 3570
    },
    {
      "epoch": 0.4071492797765228,
      "grad_norm": 0.703530490398407,
      "learning_rate": 0.00017787761601455868,
      "loss": 1.5141,
      "step": 3580
    },
    {
      "epoch": 0.4082865682675187,
      "grad_norm": 0.6438563466072083,
      "learning_rate": 0.0001775363967242948,
      "loss": 1.4827,
      "step": 3590
    },
    {
      "epoch": 0.40942385675851456,
      "grad_norm": 0.5915114283561707,
      "learning_rate": 0.00017719517743403093,
      "loss": 1.5247,
      "step": 3600
    },
    {
      "epoch": 0.41056114524951043,
      "grad_norm": 0.56954026222229,
      "learning_rate": 0.00017685395814376703,
      "loss": 1.5045,
      "step": 3610
    },
    {
      "epoch": 0.4116984337405063,
      "grad_norm": 0.5664401650428772,
      "learning_rate": 0.00017651273885350315,
      "loss": 1.5504,
      "step": 3620
    },
    {
      "epoch": 0.41283572223150217,
      "grad_norm": 0.7159045934677124,
      "learning_rate": 0.0001761715195632393,
      "loss": 1.5268,
      "step": 3630
    },
    {
      "epoch": 0.41397301072249804,
      "grad_norm": 0.5951171517372131,
      "learning_rate": 0.00017583030027297543,
      "loss": 1.5061,
      "step": 3640
    },
    {
      "epoch": 0.4151102992134939,
      "grad_norm": 0.5857769250869751,
      "learning_rate": 0.00017548908098271153,
      "loss": 1.5096,
      "step": 3650
    },
    {
      "epoch": 0.4162475877044898,
      "grad_norm": 0.6658411026000977,
      "learning_rate": 0.00017514786169244766,
      "loss": 1.5786,
      "step": 3660
    },
    {
      "epoch": 0.4173848761954857,
      "grad_norm": 0.6229246854782104,
      "learning_rate": 0.00017480664240218378,
      "loss": 1.5115,
      "step": 3670
    },
    {
      "epoch": 0.41852216468648157,
      "grad_norm": 0.6953864097595215,
      "learning_rate": 0.00017446542311191994,
      "loss": 1.5206,
      "step": 3680
    },
    {
      "epoch": 0.41965945317747744,
      "grad_norm": 0.5945801138877869,
      "learning_rate": 0.00017412420382165603,
      "loss": 1.5141,
      "step": 3690
    },
    {
      "epoch": 0.4207967416684733,
      "grad_norm": 0.5867582559585571,
      "learning_rate": 0.00017378298453139216,
      "loss": 1.4923,
      "step": 3700
    },
    {
      "epoch": 0.4219340301594692,
      "grad_norm": 0.7037670612335205,
      "learning_rate": 0.00017344176524112829,
      "loss": 1.5249,
      "step": 3710
    },
    {
      "epoch": 0.42307131865046504,
      "grad_norm": 0.5913829207420349,
      "learning_rate": 0.0001731005459508644,
      "loss": 1.5667,
      "step": 3720
    },
    {
      "epoch": 0.4242086071414609,
      "grad_norm": 0.7200407981872559,
      "learning_rate": 0.00017275932666060054,
      "loss": 1.5485,
      "step": 3730
    },
    {
      "epoch": 0.4253458956324568,
      "grad_norm": 0.5931691527366638,
      "learning_rate": 0.00017241810737033666,
      "loss": 1.4833,
      "step": 3740
    },
    {
      "epoch": 0.42648318412345265,
      "grad_norm": 0.676021933555603,
      "learning_rate": 0.00017207688808007276,
      "loss": 1.596,
      "step": 3750
    },
    {
      "epoch": 0.4276204726144485,
      "grad_norm": 0.5866082310676575,
      "learning_rate": 0.0001717356687898089,
      "loss": 1.5441,
      "step": 3760
    },
    {
      "epoch": 0.4287577611054444,
      "grad_norm": 0.6059371829032898,
      "learning_rate": 0.00017139444949954504,
      "loss": 1.543,
      "step": 3770
    },
    {
      "epoch": 0.42989504959644026,
      "grad_norm": 0.6130224466323853,
      "learning_rate": 0.00017105323020928117,
      "loss": 1.5488,
      "step": 3780
    },
    {
      "epoch": 0.4310323380874362,
      "grad_norm": 0.7205840945243835,
      "learning_rate": 0.00017071201091901727,
      "loss": 1.5892,
      "step": 3790
    },
    {
      "epoch": 0.43216962657843205,
      "grad_norm": 0.6057532429695129,
      "learning_rate": 0.0001703707916287534,
      "loss": 1.5215,
      "step": 3800
    },
    {
      "epoch": 0.4333069150694279,
      "grad_norm": 0.7254530191421509,
      "learning_rate": 0.00017002957233848952,
      "loss": 1.5022,
      "step": 3810
    },
    {
      "epoch": 0.4344442035604238,
      "grad_norm": 0.549281120300293,
      "learning_rate": 0.00016968835304822564,
      "loss": 1.5918,
      "step": 3820
    },
    {
      "epoch": 0.43558149205141966,
      "grad_norm": 0.5855221152305603,
      "learning_rate": 0.00016934713375796177,
      "loss": 1.4835,
      "step": 3830
    },
    {
      "epoch": 0.43671878054241553,
      "grad_norm": 0.5829431414604187,
      "learning_rate": 0.0001690059144676979,
      "loss": 1.5552,
      "step": 3840
    },
    {
      "epoch": 0.4378560690334114,
      "grad_norm": 0.6519951224327087,
      "learning_rate": 0.000168664695177434,
      "loss": 1.5376,
      "step": 3850
    },
    {
      "epoch": 0.43899335752440727,
      "grad_norm": 0.6808171272277832,
      "learning_rate": 0.00016832347588717015,
      "loss": 1.5416,
      "step": 3860
    },
    {
      "epoch": 0.44013064601540314,
      "grad_norm": 0.6443933248519897,
      "learning_rate": 0.00016798225659690627,
      "loss": 1.5152,
      "step": 3870
    },
    {
      "epoch": 0.441267934506399,
      "grad_norm": 0.5973288416862488,
      "learning_rate": 0.0001676410373066424,
      "loss": 1.5613,
      "step": 3880
    },
    {
      "epoch": 0.4424052229973949,
      "grad_norm": 0.6105381846427917,
      "learning_rate": 0.0001672998180163785,
      "loss": 1.4976,
      "step": 3890
    },
    {
      "epoch": 0.44354251148839074,
      "grad_norm": 0.6491042375564575,
      "learning_rate": 0.00016695859872611462,
      "loss": 1.5838,
      "step": 3900
    },
    {
      "epoch": 0.44467979997938667,
      "grad_norm": 0.6480357050895691,
      "learning_rate": 0.00016661737943585078,
      "loss": 1.5115,
      "step": 3910
    },
    {
      "epoch": 0.44581708847038254,
      "grad_norm": 0.5999581217765808,
      "learning_rate": 0.0001662761601455869,
      "loss": 1.5226,
      "step": 3920
    },
    {
      "epoch": 0.4469543769613784,
      "grad_norm": 0.6908693909645081,
      "learning_rate": 0.000165934940855323,
      "loss": 1.5147,
      "step": 3930
    },
    {
      "epoch": 0.4480916654523743,
      "grad_norm": 0.6137966513633728,
      "learning_rate": 0.00016559372156505913,
      "loss": 1.5034,
      "step": 3940
    },
    {
      "epoch": 0.44922895394337015,
      "grad_norm": 0.6844719052314758,
      "learning_rate": 0.00016525250227479525,
      "loss": 1.5531,
      "step": 3950
    },
    {
      "epoch": 0.450366242434366,
      "grad_norm": 0.6987674236297607,
      "learning_rate": 0.00016491128298453138,
      "loss": 1.4984,
      "step": 3960
    },
    {
      "epoch": 0.4515035309253619,
      "grad_norm": 0.655571460723877,
      "learning_rate": 0.0001645700636942675,
      "loss": 1.5939,
      "step": 3970
    },
    {
      "epoch": 0.45264081941635775,
      "grad_norm": 0.6564264893531799,
      "learning_rate": 0.00016422884440400363,
      "loss": 1.5559,
      "step": 3980
    },
    {
      "epoch": 0.4537781079073536,
      "grad_norm": 0.6542285084724426,
      "learning_rate": 0.00016388762511373973,
      "loss": 1.5123,
      "step": 3990
    },
    {
      "epoch": 0.4549153963983495,
      "grad_norm": 0.707380473613739,
      "learning_rate": 0.00016354640582347585,
      "loss": 1.6017,
      "step": 4000
    },
    {
      "epoch": 0.45605268488934536,
      "grad_norm": 0.5657899379730225,
      "learning_rate": 0.000163205186533212,
      "loss": 1.4949,
      "step": 4010
    },
    {
      "epoch": 0.45718997338034123,
      "grad_norm": 0.6470931172370911,
      "learning_rate": 0.00016286396724294813,
      "loss": 1.5482,
      "step": 4020
    },
    {
      "epoch": 0.45832726187133715,
      "grad_norm": 0.6612014174461365,
      "learning_rate": 0.00016252274795268423,
      "loss": 1.5407,
      "step": 4030
    },
    {
      "epoch": 0.459464550362333,
      "grad_norm": 0.6357887983322144,
      "learning_rate": 0.00016218152866242036,
      "loss": 1.4905,
      "step": 4040
    },
    {
      "epoch": 0.4606018388533289,
      "grad_norm": 0.5418158173561096,
      "learning_rate": 0.0001618403093721565,
      "loss": 1.523,
      "step": 4050
    },
    {
      "epoch": 0.46173912734432476,
      "grad_norm": 0.7082570791244507,
      "learning_rate": 0.00016149909008189264,
      "loss": 1.5268,
      "step": 4060
    },
    {
      "epoch": 0.46287641583532063,
      "grad_norm": 0.6742441654205322,
      "learning_rate": 0.00016115787079162873,
      "loss": 1.5334,
      "step": 4070
    },
    {
      "epoch": 0.4640137043263165,
      "grad_norm": 0.5844184756278992,
      "learning_rate": 0.00016081665150136486,
      "loss": 1.4734,
      "step": 4080
    },
    {
      "epoch": 0.46515099281731237,
      "grad_norm": 0.5842256546020508,
      "learning_rate": 0.00016047543221110099,
      "loss": 1.542,
      "step": 4090
    },
    {
      "epoch": 0.46628828130830824,
      "grad_norm": 0.6796230673789978,
      "learning_rate": 0.0001601342129208371,
      "loss": 1.5185,
      "step": 4100
    },
    {
      "epoch": 0.4674255697993041,
      "grad_norm": 0.6344736218452454,
      "learning_rate": 0.00015979299363057324,
      "loss": 1.5105,
      "step": 4110
    },
    {
      "epoch": 0.4685628582903,
      "grad_norm": 0.5819907188415527,
      "learning_rate": 0.00015945177434030936,
      "loss": 1.4972,
      "step": 4120
    },
    {
      "epoch": 0.46970014678129585,
      "grad_norm": 0.6378006935119629,
      "learning_rate": 0.00015911055505004546,
      "loss": 1.544,
      "step": 4130
    },
    {
      "epoch": 0.4708374352722917,
      "grad_norm": 0.5922215580940247,
      "learning_rate": 0.0001587693357597816,
      "loss": 1.5416,
      "step": 4140
    },
    {
      "epoch": 0.47197472376328764,
      "grad_norm": 0.6157101392745972,
      "learning_rate": 0.00015842811646951774,
      "loss": 1.5186,
      "step": 4150
    },
    {
      "epoch": 0.4731120122542835,
      "grad_norm": 0.6367439031600952,
      "learning_rate": 0.00015808689717925387,
      "loss": 1.5165,
      "step": 4160
    },
    {
      "epoch": 0.4742493007452794,
      "grad_norm": 0.6269174218177795,
      "learning_rate": 0.00015774567788898997,
      "loss": 1.5003,
      "step": 4170
    },
    {
      "epoch": 0.47538658923627525,
      "grad_norm": 0.6501245498657227,
      "learning_rate": 0.0001574044585987261,
      "loss": 1.5318,
      "step": 4180
    },
    {
      "epoch": 0.4765238777272711,
      "grad_norm": 0.6807112693786621,
      "learning_rate": 0.00015706323930846224,
      "loss": 1.5061,
      "step": 4190
    },
    {
      "epoch": 0.477661166218267,
      "grad_norm": 0.6177850365638733,
      "learning_rate": 0.00015672202001819837,
      "loss": 1.5307,
      "step": 4200
    },
    {
      "epoch": 0.47879845470926286,
      "grad_norm": 0.5740458369255066,
      "learning_rate": 0.00015638080072793447,
      "loss": 1.5146,
      "step": 4210
    },
    {
      "epoch": 0.4799357432002587,
      "grad_norm": 0.6377936601638794,
      "learning_rate": 0.0001560395814376706,
      "loss": 1.5454,
      "step": 4220
    },
    {
      "epoch": 0.4810730316912546,
      "grad_norm": 0.6005398035049438,
      "learning_rate": 0.00015569836214740672,
      "loss": 1.5441,
      "step": 4230
    },
    {
      "epoch": 0.48221032018225046,
      "grad_norm": 0.5819258689880371,
      "learning_rate": 0.00015535714285714285,
      "loss": 1.473,
      "step": 4240
    },
    {
      "epoch": 0.48334760867324633,
      "grad_norm": 0.7443731427192688,
      "learning_rate": 0.00015501592356687897,
      "loss": 1.5109,
      "step": 4250
    },
    {
      "epoch": 0.4844848971642422,
      "grad_norm": 0.6243463158607483,
      "learning_rate": 0.0001546747042766151,
      "loss": 1.5269,
      "step": 4260
    },
    {
      "epoch": 0.4856221856552381,
      "grad_norm": 0.5822690725326538,
      "learning_rate": 0.0001543676069153776,
      "loss": 1.53,
      "step": 4270
    },
    {
      "epoch": 0.486759474146234,
      "grad_norm": 0.5915675759315491,
      "learning_rate": 0.00015402638762511373,
      "loss": 1.539,
      "step": 4280
    },
    {
      "epoch": 0.48789676263722986,
      "grad_norm": 0.6022122502326965,
      "learning_rate": 0.00015368516833484983,
      "loss": 1.5115,
      "step": 4290
    },
    {
      "epoch": 0.48903405112822573,
      "grad_norm": 0.6099061369895935,
      "learning_rate": 0.00015334394904458599,
      "loss": 1.4929,
      "step": 4300
    },
    {
      "epoch": 0.4901713396192216,
      "grad_norm": 0.6795952320098877,
      "learning_rate": 0.0001530027297543221,
      "loss": 1.564,
      "step": 4310
    },
    {
      "epoch": 0.49130862811021747,
      "grad_norm": 0.5788513422012329,
      "learning_rate": 0.0001526615104640582,
      "loss": 1.4593,
      "step": 4320
    },
    {
      "epoch": 0.49244591660121334,
      "grad_norm": 0.5932690501213074,
      "learning_rate": 0.00015232029117379434,
      "loss": 1.4953,
      "step": 4330
    },
    {
      "epoch": 0.4935832050922092,
      "grad_norm": 0.6726161241531372,
      "learning_rate": 0.00015197907188353046,
      "loss": 1.4979,
      "step": 4340
    },
    {
      "epoch": 0.4947204935832051,
      "grad_norm": 0.6341193914413452,
      "learning_rate": 0.00015163785259326662,
      "loss": 1.488,
      "step": 4350
    },
    {
      "epoch": 0.49585778207420095,
      "grad_norm": 0.644630491733551,
      "learning_rate": 0.00015129663330300271,
      "loss": 1.5501,
      "step": 4360
    },
    {
      "epoch": 0.4969950705651968,
      "grad_norm": 0.6302149891853333,
      "learning_rate": 0.00015095541401273884,
      "loss": 1.5119,
      "step": 4370
    },
    {
      "epoch": 0.4981323590561927,
      "grad_norm": 0.666841983795166,
      "learning_rate": 0.00015061419472247497,
      "loss": 1.533,
      "step": 4380
    },
    {
      "epoch": 0.4992696475471886,
      "grad_norm": 0.6391720175743103,
      "learning_rate": 0.00015027297543221106,
      "loss": 1.5295,
      "step": 4390
    },
    {
      "epoch": 0.5004069360381844,
      "grad_norm": 0.6596633195877075,
      "learning_rate": 0.00014993175614194722,
      "loss": 1.5099,
      "step": 4400
    },
    {
      "epoch": 0.5015442245291803,
      "grad_norm": 0.6848667860031128,
      "learning_rate": 0.00014959053685168334,
      "loss": 1.5332,
      "step": 4410
    },
    {
      "epoch": 0.5026815130201762,
      "grad_norm": 0.6605181097984314,
      "learning_rate": 0.00014924931756141947,
      "loss": 1.5341,
      "step": 4420
    },
    {
      "epoch": 0.5038188015111721,
      "grad_norm": 0.6023310422897339,
      "learning_rate": 0.0001489080982711556,
      "loss": 1.5359,
      "step": 4430
    },
    {
      "epoch": 0.5049560900021679,
      "grad_norm": 0.5884245038032532,
      "learning_rate": 0.0001485668789808917,
      "loss": 1.4909,
      "step": 4440
    },
    {
      "epoch": 0.5060933784931638,
      "grad_norm": 0.5933159589767456,
      "learning_rate": 0.00014822565969062782,
      "loss": 1.4587,
      "step": 4450
    },
    {
      "epoch": 0.5072306669841598,
      "grad_norm": 0.588474690914154,
      "learning_rate": 0.00014788444040036395,
      "loss": 1.5703,
      "step": 4460
    },
    {
      "epoch": 0.5083679554751556,
      "grad_norm": 0.651603639125824,
      "learning_rate": 0.00014754322111010007,
      "loss": 1.5383,
      "step": 4470
    },
    {
      "epoch": 0.5095052439661515,
      "grad_norm": 0.5699765682220459,
      "learning_rate": 0.0001472020018198362,
      "loss": 1.5179,
      "step": 4480
    },
    {
      "epoch": 0.5106425324571473,
      "grad_norm": 0.6648078560829163,
      "learning_rate": 0.00014686078252957232,
      "loss": 1.534,
      "step": 4490
    },
    {
      "epoch": 0.5117798209481432,
      "grad_norm": 0.5921615362167358,
      "learning_rate": 0.00014651956323930845,
      "loss": 1.5172,
      "step": 4500
    },
    {
      "epoch": 0.512917109439139,
      "grad_norm": 0.6267717480659485,
      "learning_rate": 0.00014617834394904457,
      "loss": 1.5409,
      "step": 4510
    },
    {
      "epoch": 0.514054397930135,
      "grad_norm": 0.6602720618247986,
      "learning_rate": 0.0001458371246587807,
      "loss": 1.5444,
      "step": 4520
    },
    {
      "epoch": 0.5151916864211308,
      "grad_norm": 0.6220770478248596,
      "learning_rate": 0.00014549590536851683,
      "loss": 1.5358,
      "step": 4530
    },
    {
      "epoch": 0.5163289749121267,
      "grad_norm": 0.6846970319747925,
      "learning_rate": 0.00014515468607825295,
      "loss": 1.5814,
      "step": 4540
    },
    {
      "epoch": 0.5174662634031225,
      "grad_norm": 0.6016876101493835,
      "learning_rate": 0.00014481346678798908,
      "loss": 1.5047,
      "step": 4550
    },
    {
      "epoch": 0.5186035518941184,
      "grad_norm": 0.613744854927063,
      "learning_rate": 0.0001444722474977252,
      "loss": 1.513,
      "step": 4560
    },
    {
      "epoch": 0.5197408403851144,
      "grad_norm": 0.6846521496772766,
      "learning_rate": 0.0001441310282074613,
      "loss": 1.509,
      "step": 4570
    },
    {
      "epoch": 0.5208781288761102,
      "grad_norm": 0.5731900334358215,
      "learning_rate": 0.00014378980891719743,
      "loss": 1.5286,
      "step": 4580
    },
    {
      "epoch": 0.5220154173671061,
      "grad_norm": 0.6342533826828003,
      "learning_rate": 0.00014344858962693355,
      "loss": 1.5127,
      "step": 4590
    },
    {
      "epoch": 0.5231527058581019,
      "grad_norm": 0.576724112033844,
      "learning_rate": 0.00014310737033666968,
      "loss": 1.474,
      "step": 4600
    },
    {
      "epoch": 0.5242899943490978,
      "grad_norm": 0.6269608736038208,
      "learning_rate": 0.0001427661510464058,
      "loss": 1.5493,
      "step": 4610
    },
    {
      "epoch": 0.5254272828400937,
      "grad_norm": 0.6186097860336304,
      "learning_rate": 0.00014242493175614193,
      "loss": 1.5034,
      "step": 4620
    },
    {
      "epoch": 0.5265645713310896,
      "grad_norm": 0.5945231318473816,
      "learning_rate": 0.00014208371246587806,
      "loss": 1.5002,
      "step": 4630
    },
    {
      "epoch": 0.5277018598220854,
      "grad_norm": 0.6240324378013611,
      "learning_rate": 0.00014174249317561418,
      "loss": 1.5536,
      "step": 4640
    },
    {
      "epoch": 0.5288391483130813,
      "grad_norm": 0.7175806164741516,
      "learning_rate": 0.0001414012738853503,
      "loss": 1.5595,
      "step": 4650
    },
    {
      "epoch": 0.5299764368040771,
      "grad_norm": 0.6417214870452881,
      "learning_rate": 0.00014106005459508644,
      "loss": 1.4845,
      "step": 4660
    },
    {
      "epoch": 0.5311137252950731,
      "grad_norm": 0.6174720525741577,
      "learning_rate": 0.00014071883530482256,
      "loss": 1.4796,
      "step": 4670
    },
    {
      "epoch": 0.5322510137860689,
      "grad_norm": 0.6651455760002136,
      "learning_rate": 0.0001403776160145587,
      "loss": 1.5058,
      "step": 4680
    },
    {
      "epoch": 0.5333883022770648,
      "grad_norm": 0.6966311931610107,
      "learning_rate": 0.0001400363967242948,
      "loss": 1.4875,
      "step": 4690
    },
    {
      "epoch": 0.5345255907680607,
      "grad_norm": 0.6196454167366028,
      "learning_rate": 0.00013969517743403094,
      "loss": 1.5132,
      "step": 4700
    },
    {
      "epoch": 0.5356628792590565,
      "grad_norm": 0.7104840278625488,
      "learning_rate": 0.00013935395814376704,
      "loss": 1.5445,
      "step": 4710
    },
    {
      "epoch": 0.5368001677500525,
      "grad_norm": 0.7143914699554443,
      "learning_rate": 0.00013901273885350316,
      "loss": 1.503,
      "step": 4720
    },
    {
      "epoch": 0.5379374562410483,
      "grad_norm": 0.6447052955627441,
      "learning_rate": 0.0001386715195632393,
      "loss": 1.4612,
      "step": 4730
    },
    {
      "epoch": 0.5390747447320442,
      "grad_norm": 0.6265527606010437,
      "learning_rate": 0.00013833030027297541,
      "loss": 1.543,
      "step": 4740
    },
    {
      "epoch": 0.54021203322304,
      "grad_norm": 0.619567334651947,
      "learning_rate": 0.00013798908098271154,
      "loss": 1.5188,
      "step": 4750
    },
    {
      "epoch": 0.5413493217140359,
      "grad_norm": 0.6400176882743835,
      "learning_rate": 0.00013764786169244767,
      "loss": 1.4922,
      "step": 4760
    },
    {
      "epoch": 0.5424866102050317,
      "grad_norm": 0.5689389705657959,
      "learning_rate": 0.0001373066424021838,
      "loss": 1.4991,
      "step": 4770
    },
    {
      "epoch": 0.5436238986960277,
      "grad_norm": 0.6065521836280823,
      "learning_rate": 0.00013696542311191992,
      "loss": 1.4708,
      "step": 4780
    },
    {
      "epoch": 0.5447611871870235,
      "grad_norm": 0.638457179069519,
      "learning_rate": 0.00013662420382165604,
      "loss": 1.5573,
      "step": 4790
    },
    {
      "epoch": 0.5458984756780194,
      "grad_norm": 0.6548057794570923,
      "learning_rate": 0.00013628298453139217,
      "loss": 1.5332,
      "step": 4800
    },
    {
      "epoch": 0.5470357641690153,
      "grad_norm": 0.6843760013580322,
      "learning_rate": 0.0001359417652411283,
      "loss": 1.4858,
      "step": 4810
    },
    {
      "epoch": 0.5481730526600112,
      "grad_norm": 0.6506561636924744,
      "learning_rate": 0.00013560054595086442,
      "loss": 1.4908,
      "step": 4820
    },
    {
      "epoch": 0.5493103411510071,
      "grad_norm": 0.6796621680259705,
      "learning_rate": 0.00013525932666060055,
      "loss": 1.4873,
      "step": 4830
    },
    {
      "epoch": 0.5504476296420029,
      "grad_norm": 0.7837737798690796,
      "learning_rate": 0.00013491810737033665,
      "loss": 1.5129,
      "step": 4840
    },
    {
      "epoch": 0.5515849181329988,
      "grad_norm": 0.6507452130317688,
      "learning_rate": 0.00013457688808007277,
      "loss": 1.4486,
      "step": 4850
    },
    {
      "epoch": 0.5527222066239946,
      "grad_norm": 0.5564322471618652,
      "learning_rate": 0.0001342356687898089,
      "loss": 1.5044,
      "step": 4860
    },
    {
      "epoch": 0.5538594951149906,
      "grad_norm": 0.6681380271911621,
      "learning_rate": 0.00013389444949954502,
      "loss": 1.5547,
      "step": 4870
    },
    {
      "epoch": 0.5549967836059864,
      "grad_norm": 0.6760812401771545,
      "learning_rate": 0.00013355323020928115,
      "loss": 1.5177,
      "step": 4880
    },
    {
      "epoch": 0.5561340720969823,
      "grad_norm": 0.5995465517044067,
      "learning_rate": 0.00013321201091901728,
      "loss": 1.576,
      "step": 4890
    },
    {
      "epoch": 0.5572713605879781,
      "grad_norm": 0.6401102542877197,
      "learning_rate": 0.0001328707916287534,
      "loss": 1.539,
      "step": 4900
    },
    {
      "epoch": 0.558408649078974,
      "grad_norm": 0.6074585914611816,
      "learning_rate": 0.00013252957233848953,
      "loss": 1.5269,
      "step": 4910
    },
    {
      "epoch": 0.5595459375699698,
      "grad_norm": 0.5766951441764832,
      "learning_rate": 0.00013218835304822565,
      "loss": 1.4984,
      "step": 4920
    },
    {
      "epoch": 0.5606832260609658,
      "grad_norm": 0.631747841835022,
      "learning_rate": 0.00013184713375796178,
      "loss": 1.5121,
      "step": 4930
    },
    {
      "epoch": 0.5618205145519617,
      "grad_norm": 0.7305492758750916,
      "learning_rate": 0.0001315059144676979,
      "loss": 1.5322,
      "step": 4940
    },
    {
      "epoch": 0.5629578030429575,
      "grad_norm": 0.61118483543396,
      "learning_rate": 0.00013116469517743403,
      "loss": 1.5706,
      "step": 4950
    },
    {
      "epoch": 0.5640950915339534,
      "grad_norm": 0.6287723183631897,
      "learning_rate": 0.00013082347588717016,
      "loss": 1.5462,
      "step": 4960
    },
    {
      "epoch": 0.5652323800249492,
      "grad_norm": 0.6552715301513672,
      "learning_rate": 0.00013048225659690625,
      "loss": 1.5205,
      "step": 4970
    },
    {
      "epoch": 0.5663696685159452,
      "grad_norm": 0.6365973949432373,
      "learning_rate": 0.00013014103730664238,
      "loss": 1.532,
      "step": 4980
    },
    {
      "epoch": 0.567506957006941,
      "grad_norm": 0.6326168775558472,
      "learning_rate": 0.0001297998180163785,
      "loss": 1.5259,
      "step": 4990
    },
    {
      "epoch": 0.5686442454979369,
      "grad_norm": 0.6296125054359436,
      "learning_rate": 0.00012945859872611463,
      "loss": 1.5273,
      "step": 5000
    },
    {
      "epoch": 0.5697815339889327,
      "grad_norm": 0.6219555139541626,
      "learning_rate": 0.00012911737943585076,
      "loss": 1.5675,
      "step": 5010
    },
    {
      "epoch": 0.5709188224799286,
      "grad_norm": 0.5555018186569214,
      "learning_rate": 0.00012877616014558688,
      "loss": 1.4825,
      "step": 5020
    },
    {
      "epoch": 0.5720561109709245,
      "grad_norm": 0.622111439704895,
      "learning_rate": 0.000128434940855323,
      "loss": 1.5526,
      "step": 5030
    },
    {
      "epoch": 0.5731933994619204,
      "grad_norm": 0.6105301976203918,
      "learning_rate": 0.00012809372156505914,
      "loss": 1.5524,
      "step": 5040
    },
    {
      "epoch": 0.5743306879529163,
      "grad_norm": 0.7641129493713379,
      "learning_rate": 0.00012775250227479526,
      "loss": 1.5348,
      "step": 5050
    },
    {
      "epoch": 0.5754679764439121,
      "grad_norm": 0.599059522151947,
      "learning_rate": 0.0001274112829845314,
      "loss": 1.5067,
      "step": 5060
    },
    {
      "epoch": 0.576605264934908,
      "grad_norm": 0.6049686074256897,
      "learning_rate": 0.0001270700636942675,
      "loss": 1.4861,
      "step": 5070
    },
    {
      "epoch": 0.5777425534259039,
      "grad_norm": 0.665040135383606,
      "learning_rate": 0.00012672884440400364,
      "loss": 1.4806,
      "step": 5080
    },
    {
      "epoch": 0.5788798419168998,
      "grad_norm": 0.6373928785324097,
      "learning_rate": 0.00012638762511373976,
      "loss": 1.4783,
      "step": 5090
    },
    {
      "epoch": 0.5800171304078956,
      "grad_norm": 0.6039661169052124,
      "learning_rate": 0.00012604640582347586,
      "loss": 1.5123,
      "step": 5100
    },
    {
      "epoch": 0.5811544188988915,
      "grad_norm": 0.5980367064476013,
      "learning_rate": 0.000125705186533212,
      "loss": 1.5569,
      "step": 5110
    },
    {
      "epoch": 0.5822917073898873,
      "grad_norm": 0.6498721837997437,
      "learning_rate": 0.00012536396724294812,
      "loss": 1.5579,
      "step": 5120
    },
    {
      "epoch": 0.5834289958808833,
      "grad_norm": 0.6832205057144165,
      "learning_rate": 0.00012502274795268424,
      "loss": 1.481,
      "step": 5130
    },
    {
      "epoch": 0.5845662843718791,
      "grad_norm": 0.5783844590187073,
      "learning_rate": 0.00012468152866242037,
      "loss": 1.5359,
      "step": 5140
    },
    {
      "epoch": 0.585703572862875,
      "grad_norm": 0.5713112354278564,
      "learning_rate": 0.0001243403093721565,
      "loss": 1.551,
      "step": 5150
    },
    {
      "epoch": 0.5868408613538708,
      "grad_norm": 0.6046285629272461,
      "learning_rate": 0.00012399909008189262,
      "loss": 1.4962,
      "step": 5160
    },
    {
      "epoch": 0.5879781498448667,
      "grad_norm": 0.6730244755744934,
      "learning_rate": 0.00012365787079162874,
      "loss": 1.5357,
      "step": 5170
    },
    {
      "epoch": 0.5891154383358627,
      "grad_norm": 0.5619633793830872,
      "learning_rate": 0.00012331665150136487,
      "loss": 1.5225,
      "step": 5180
    },
    {
      "epoch": 0.5902527268268585,
      "grad_norm": 0.6988450288772583,
      "learning_rate": 0.000122975432211101,
      "loss": 1.5229,
      "step": 5190
    },
    {
      "epoch": 0.5913900153178544,
      "grad_norm": 0.627635657787323,
      "learning_rate": 0.00012263421292083712,
      "loss": 1.4765,
      "step": 5200
    },
    {
      "epoch": 0.5925273038088502,
      "grad_norm": 0.6095670461654663,
      "learning_rate": 0.00012229299363057325,
      "loss": 1.4496,
      "step": 5210
    },
    {
      "epoch": 0.5936645922998461,
      "grad_norm": 0.6163301467895508,
      "learning_rate": 0.00012195177434030936,
      "loss": 1.4719,
      "step": 5220
    },
    {
      "epoch": 0.594801880790842,
      "grad_norm": 0.6544303894042969,
      "learning_rate": 0.00012161055505004547,
      "loss": 1.5078,
      "step": 5230
    },
    {
      "epoch": 0.5959391692818379,
      "grad_norm": 0.6784412860870361,
      "learning_rate": 0.00012126933575978161,
      "loss": 1.5425,
      "step": 5240
    },
    {
      "epoch": 0.5970764577728337,
      "grad_norm": 0.6379384994506836,
      "learning_rate": 0.00012092811646951772,
      "loss": 1.5131,
      "step": 5250
    },
    {
      "epoch": 0.5982137462638296,
      "grad_norm": 0.6230953335762024,
      "learning_rate": 0.00012058689717925386,
      "loss": 1.5104,
      "step": 5260
    },
    {
      "epoch": 0.5993510347548254,
      "grad_norm": 0.6117348074913025,
      "learning_rate": 0.00012024567788898998,
      "loss": 1.4436,
      "step": 5270
    },
    {
      "epoch": 0.6004883232458214,
      "grad_norm": 0.6574910283088684,
      "learning_rate": 0.00011990445859872612,
      "loss": 1.5571,
      "step": 5280
    },
    {
      "epoch": 0.6016256117368173,
      "grad_norm": 0.6377577781677246,
      "learning_rate": 0.00011956323930846223,
      "loss": 1.5529,
      "step": 5290
    },
    {
      "epoch": 0.6027629002278131,
      "grad_norm": 0.6594663262367249,
      "learning_rate": 0.00011922202001819834,
      "loss": 1.4461,
      "step": 5300
    },
    {
      "epoch": 0.603900188718809,
      "grad_norm": 0.6271266341209412,
      "learning_rate": 0.00011888080072793448,
      "loss": 1.5026,
      "step": 5310
    },
    {
      "epoch": 0.6050374772098048,
      "grad_norm": 0.5915992856025696,
      "learning_rate": 0.00011853958143767059,
      "loss": 1.493,
      "step": 5320
    },
    {
      "epoch": 0.6061747657008008,
      "grad_norm": 0.6268733739852905,
      "learning_rate": 0.00011819836214740673,
      "loss": 1.4641,
      "step": 5330
    },
    {
      "epoch": 0.6073120541917966,
      "grad_norm": 0.6265775561332703,
      "learning_rate": 0.00011785714285714284,
      "loss": 1.5112,
      "step": 5340
    },
    {
      "epoch": 0.6084493426827925,
      "grad_norm": 0.5766099691390991,
      "learning_rate": 0.00011751592356687898,
      "loss": 1.5151,
      "step": 5350
    },
    {
      "epoch": 0.6095866311737883,
      "grad_norm": 0.5879406332969666,
      "learning_rate": 0.0001171747042766151,
      "loss": 1.4665,
      "step": 5360
    },
    {
      "epoch": 0.6107239196647842,
      "grad_norm": 0.8551739454269409,
      "learning_rate": 0.00011683348498635121,
      "loss": 1.5172,
      "step": 5370
    },
    {
      "epoch": 0.61186120815578,
      "grad_norm": 0.635563850402832,
      "learning_rate": 0.00011649226569608735,
      "loss": 1.5448,
      "step": 5380
    },
    {
      "epoch": 0.612998496646776,
      "grad_norm": 0.6327351927757263,
      "learning_rate": 0.00011615104640582346,
      "loss": 1.5823,
      "step": 5390
    },
    {
      "epoch": 0.6141357851377718,
      "grad_norm": 0.5609679222106934,
      "learning_rate": 0.0001158098271155596,
      "loss": 1.4815,
      "step": 5400
    },
    {
      "epoch": 0.6152730736287677,
      "grad_norm": 0.6231576204299927,
      "learning_rate": 0.00011546860782529571,
      "loss": 1.5073,
      "step": 5410
    },
    {
      "epoch": 0.6164103621197636,
      "grad_norm": 0.6367419958114624,
      "learning_rate": 0.00011512738853503185,
      "loss": 1.4779,
      "step": 5420
    },
    {
      "epoch": 0.6175476506107594,
      "grad_norm": 0.6242161393165588,
      "learning_rate": 0.00011478616924476796,
      "loss": 1.4879,
      "step": 5430
    },
    {
      "epoch": 0.6186849391017554,
      "grad_norm": 0.6101197600364685,
      "learning_rate": 0.00011444494995450407,
      "loss": 1.5591,
      "step": 5440
    },
    {
      "epoch": 0.6198222275927512,
      "grad_norm": 0.605401873588562,
      "learning_rate": 0.00011410373066424021,
      "loss": 1.4848,
      "step": 5450
    },
    {
      "epoch": 0.6209595160837471,
      "grad_norm": 0.6132580041885376,
      "learning_rate": 0.00011376251137397633,
      "loss": 1.4982,
      "step": 5460
    },
    {
      "epoch": 0.6220968045747429,
      "grad_norm": 0.5749717950820923,
      "learning_rate": 0.00011342129208371247,
      "loss": 1.5454,
      "step": 5470
    },
    {
      "epoch": 0.6232340930657388,
      "grad_norm": 0.6327801942825317,
      "learning_rate": 0.00011308007279344858,
      "loss": 1.5243,
      "step": 5480
    },
    {
      "epoch": 0.6243713815567347,
      "grad_norm": 0.5694487690925598,
      "learning_rate": 0.00011273885350318472,
      "loss": 1.5015,
      "step": 5490
    },
    {
      "epoch": 0.6255086700477306,
      "grad_norm": 0.5931757092475891,
      "learning_rate": 0.00011239763421292083,
      "loss": 1.5157,
      "step": 5500
    },
    {
      "epoch": 0.6266459585387264,
      "grad_norm": 0.7476601600646973,
      "learning_rate": 0.00011205641492265694,
      "loss": 1.4861,
      "step": 5510
    },
    {
      "epoch": 0.6277832470297223,
      "grad_norm": 0.6356845498085022,
      "learning_rate": 0.00011171519563239308,
      "loss": 1.5173,
      "step": 5520
    },
    {
      "epoch": 0.6289205355207182,
      "grad_norm": 0.6304683089256287,
      "learning_rate": 0.00011137397634212919,
      "loss": 1.4588,
      "step": 5530
    },
    {
      "epoch": 0.6300578240117141,
      "grad_norm": 0.6556190252304077,
      "learning_rate": 0.00011103275705186533,
      "loss": 1.5446,
      "step": 5540
    },
    {
      "epoch": 0.63119511250271,
      "grad_norm": 0.6194158792495728,
      "learning_rate": 0.00011069153776160144,
      "loss": 1.5216,
      "step": 5550
    },
    {
      "epoch": 0.6323324009937058,
      "grad_norm": 0.6769715547561646,
      "learning_rate": 0.00011035031847133756,
      "loss": 1.5106,
      "step": 5560
    },
    {
      "epoch": 0.6334696894847017,
      "grad_norm": 0.6211738586425781,
      "learning_rate": 0.0001100090991810737,
      "loss": 1.5462,
      "step": 5570
    },
    {
      "epoch": 0.6346069779756975,
      "grad_norm": 0.6358450651168823,
      "learning_rate": 0.00010966787989080981,
      "loss": 1.5394,
      "step": 5580
    },
    {
      "epoch": 0.6357442664666935,
      "grad_norm": 0.6169177293777466,
      "learning_rate": 0.00010932666060054595,
      "loss": 1.517,
      "step": 5590
    },
    {
      "epoch": 0.6368815549576893,
      "grad_norm": 0.5652125477790833,
      "learning_rate": 0.00010898544131028206,
      "loss": 1.5183,
      "step": 5600
    },
    {
      "epoch": 0.6380188434486852,
      "grad_norm": 0.6499515175819397,
      "learning_rate": 0.0001086442220200182,
      "loss": 1.4904,
      "step": 5610
    },
    {
      "epoch": 0.639156131939681,
      "grad_norm": 0.623026967048645,
      "learning_rate": 0.00010830300272975431,
      "loss": 1.49,
      "step": 5620
    },
    {
      "epoch": 0.6402934204306769,
      "grad_norm": 0.6040289998054504,
      "learning_rate": 0.00010796178343949042,
      "loss": 1.5274,
      "step": 5630
    },
    {
      "epoch": 0.6414307089216728,
      "grad_norm": 0.6511864066123962,
      "learning_rate": 0.00010762056414922656,
      "loss": 1.5463,
      "step": 5640
    },
    {
      "epoch": 0.6425679974126687,
      "grad_norm": 0.5859230160713196,
      "learning_rate": 0.00010727934485896268,
      "loss": 1.5145,
      "step": 5650
    },
    {
      "epoch": 0.6437052859036646,
      "grad_norm": 0.6060872077941895,
      "learning_rate": 0.00010693812556869882,
      "loss": 1.5299,
      "step": 5660
    },
    {
      "epoch": 0.6448425743946604,
      "grad_norm": 0.6507803201675415,
      "learning_rate": 0.00010659690627843493,
      "loss": 1.4752,
      "step": 5670
    },
    {
      "epoch": 0.6459798628856563,
      "grad_norm": 0.5711724162101746,
      "learning_rate": 0.00010625568698817107,
      "loss": 1.5365,
      "step": 5680
    },
    {
      "epoch": 0.6471171513766522,
      "grad_norm": 0.6112639307975769,
      "learning_rate": 0.00010591446769790718,
      "loss": 1.5376,
      "step": 5690
    },
    {
      "epoch": 0.6482544398676481,
      "grad_norm": 0.6338860988616943,
      "learning_rate": 0.00010557324840764329,
      "loss": 1.4831,
      "step": 5700
    },
    {
      "epoch": 0.6493917283586439,
      "grad_norm": 0.6063889861106873,
      "learning_rate": 0.00010523202911737943,
      "loss": 1.5512,
      "step": 5710
    },
    {
      "epoch": 0.6505290168496398,
      "grad_norm": 0.5985842943191528,
      "learning_rate": 0.00010489080982711554,
      "loss": 1.514,
      "step": 5720
    },
    {
      "epoch": 0.6516663053406356,
      "grad_norm": 0.6117998957633972,
      "learning_rate": 0.00010454959053685168,
      "loss": 1.4727,
      "step": 5730
    },
    {
      "epoch": 0.6528035938316316,
      "grad_norm": 0.7130334973335266,
      "learning_rate": 0.0001042083712465878,
      "loss": 1.5656,
      "step": 5740
    },
    {
      "epoch": 0.6539408823226274,
      "grad_norm": 0.5935847163200378,
      "learning_rate": 0.00010386715195632393,
      "loss": 1.5133,
      "step": 5750
    },
    {
      "epoch": 0.6550781708136233,
      "grad_norm": 0.6551563143730164,
      "learning_rate": 0.00010352593266606005,
      "loss": 1.4776,
      "step": 5760
    },
    {
      "epoch": 0.6562154593046192,
      "grad_norm": 0.7124812602996826,
      "learning_rate": 0.00010318471337579616,
      "loss": 1.4903,
      "step": 5770
    },
    {
      "epoch": 0.657352747795615,
      "grad_norm": 0.5732616782188416,
      "learning_rate": 0.0001028434940855323,
      "loss": 1.5422,
      "step": 5780
    },
    {
      "epoch": 0.658490036286611,
      "grad_norm": 0.6213379502296448,
      "learning_rate": 0.00010250227479526841,
      "loss": 1.523,
      "step": 5790
    },
    {
      "epoch": 0.6596273247776068,
      "grad_norm": 0.6657412648200989,
      "learning_rate": 0.00010216105550500455,
      "loss": 1.5167,
      "step": 5800
    },
    {
      "epoch": 0.6607646132686027,
      "grad_norm": 0.6200246214866638,
      "learning_rate": 0.00010181983621474066,
      "loss": 1.4996,
      "step": 5810
    },
    {
      "epoch": 0.6619019017595985,
      "grad_norm": 0.652990996837616,
      "learning_rate": 0.00010147861692447677,
      "loss": 1.5549,
      "step": 5820
    },
    {
      "epoch": 0.6630391902505944,
      "grad_norm": 0.7941602468490601,
      "learning_rate": 0.00010113739763421291,
      "loss": 1.5094,
      "step": 5830
    },
    {
      "epoch": 0.6641764787415902,
      "grad_norm": 0.6326481103897095,
      "learning_rate": 0.00010079617834394903,
      "loss": 1.5066,
      "step": 5840
    },
    {
      "epoch": 0.6653137672325862,
      "grad_norm": 0.6461867690086365,
      "learning_rate": 0.00010045495905368517,
      "loss": 1.5038,
      "step": 5850
    },
    {
      "epoch": 0.666451055723582,
      "grad_norm": 0.5855443477630615,
      "learning_rate": 0.00010011373976342128,
      "loss": 1.4863,
      "step": 5860
    },
    {
      "epoch": 0.6675883442145779,
      "grad_norm": 0.6224611401557922,
      "learning_rate": 9.977252047315742e-05,
      "loss": 1.5191,
      "step": 5870
    },
    {
      "epoch": 0.6687256327055737,
      "grad_norm": 0.5743132829666138,
      "learning_rate": 9.943130118289353e-05,
      "loss": 1.4654,
      "step": 5880
    },
    {
      "epoch": 0.6698629211965696,
      "grad_norm": 0.6911665797233582,
      "learning_rate": 9.909008189262964e-05,
      "loss": 1.5198,
      "step": 5890
    },
    {
      "epoch": 0.6710002096875656,
      "grad_norm": 0.5786201357841492,
      "learning_rate": 9.874886260236578e-05,
      "loss": 1.4906,
      "step": 5900
    },
    {
      "epoch": 0.6721374981785614,
      "grad_norm": 0.6290716528892517,
      "learning_rate": 9.84076433121019e-05,
      "loss": 1.5157,
      "step": 5910
    },
    {
      "epoch": 0.6732747866695573,
      "grad_norm": 0.6257844567298889,
      "learning_rate": 9.806642402183803e-05,
      "loss": 1.5197,
      "step": 5920
    },
    {
      "epoch": 0.6744120751605531,
      "grad_norm": 0.6641518473625183,
      "learning_rate": 9.772520473157415e-05,
      "loss": 1.5658,
      "step": 5930
    },
    {
      "epoch": 0.675549363651549,
      "grad_norm": 0.6044304370880127,
      "learning_rate": 9.738398544131028e-05,
      "loss": 1.5147,
      "step": 5940
    },
    {
      "epoch": 0.6766866521425449,
      "grad_norm": 0.5807651877403259,
      "learning_rate": 9.70427661510464e-05,
      "loss": 1.5015,
      "step": 5950
    },
    {
      "epoch": 0.6778239406335408,
      "grad_norm": 0.6012964844703674,
      "learning_rate": 9.670154686078251e-05,
      "loss": 1.5092,
      "step": 5960
    },
    {
      "epoch": 0.6789612291245366,
      "grad_norm": 0.6218732595443726,
      "learning_rate": 9.636032757051865e-05,
      "loss": 1.5762,
      "step": 5970
    },
    {
      "epoch": 0.6800985176155325,
      "grad_norm": 0.6700950264930725,
      "learning_rate": 9.601910828025476e-05,
      "loss": 1.5225,
      "step": 5980
    },
    {
      "epoch": 0.6812358061065283,
      "grad_norm": 0.6121432781219482,
      "learning_rate": 9.56778889899909e-05,
      "loss": 1.5011,
      "step": 5990
    },
    {
      "epoch": 0.6823730945975243,
      "grad_norm": 0.631524920463562,
      "learning_rate": 9.533666969972701e-05,
      "loss": 1.4579,
      "step": 6000
    },
    {
      "epoch": 0.6835103830885202,
      "grad_norm": 0.6205111742019653,
      "learning_rate": 9.499545040946315e-05,
      "loss": 1.4974,
      "step": 6010
    },
    {
      "epoch": 0.684647671579516,
      "grad_norm": 0.6385579705238342,
      "learning_rate": 9.465423111919926e-05,
      "loss": 1.5463,
      "step": 6020
    },
    {
      "epoch": 0.6857849600705119,
      "grad_norm": 0.6425485610961914,
      "learning_rate": 9.431301182893538e-05,
      "loss": 1.5124,
      "step": 6030
    },
    {
      "epoch": 0.6869222485615077,
      "grad_norm": 0.6306208968162537,
      "learning_rate": 9.397179253867152e-05,
      "loss": 1.4707,
      "step": 6040
    },
    {
      "epoch": 0.6880595370525037,
      "grad_norm": 0.6425096392631531,
      "learning_rate": 9.363057324840763e-05,
      "loss": 1.4915,
      "step": 6050
    },
    {
      "epoch": 0.6891968255434995,
      "grad_norm": 0.6909633278846741,
      "learning_rate": 9.328935395814377e-05,
      "loss": 1.4814,
      "step": 6060
    },
    {
      "epoch": 0.6903341140344954,
      "grad_norm": 0.5685998797416687,
      "learning_rate": 9.294813466787988e-05,
      "loss": 1.5162,
      "step": 6070
    },
    {
      "epoch": 0.6914714025254912,
      "grad_norm": 0.5948916673660278,
      "learning_rate": 9.260691537761602e-05,
      "loss": 1.469,
      "step": 6080
    },
    {
      "epoch": 0.6926086910164871,
      "grad_norm": 0.6221314072608948,
      "learning_rate": 9.226569608735213e-05,
      "loss": 1.4725,
      "step": 6090
    },
    {
      "epoch": 0.693745979507483,
      "grad_norm": 0.5722830891609192,
      "learning_rate": 9.192447679708824e-05,
      "loss": 1.4998,
      "step": 6100
    },
    {
      "epoch": 0.6948832679984789,
      "grad_norm": 0.5870597958564758,
      "learning_rate": 9.158325750682438e-05,
      "loss": 1.5285,
      "step": 6110
    },
    {
      "epoch": 0.6960205564894747,
      "grad_norm": 0.6083294749259949,
      "learning_rate": 9.12420382165605e-05,
      "loss": 1.467,
      "step": 6120
    },
    {
      "epoch": 0.6971578449804706,
      "grad_norm": 0.9140753149986267,
      "learning_rate": 9.090081892629664e-05,
      "loss": 1.5144,
      "step": 6130
    },
    {
      "epoch": 0.6982951334714665,
      "grad_norm": 0.6303302645683289,
      "learning_rate": 9.055959963603275e-05,
      "loss": 1.4776,
      "step": 6140
    },
    {
      "epoch": 0.6994324219624624,
      "grad_norm": 0.6156527400016785,
      "learning_rate": 9.021838034576886e-05,
      "loss": 1.5266,
      "step": 6150
    },
    {
      "epoch": 0.7005697104534583,
      "grad_norm": 0.5784417390823364,
      "learning_rate": 8.9877161055505e-05,
      "loss": 1.4408,
      "step": 6160
    },
    {
      "epoch": 0.7017069989444541,
      "grad_norm": 0.6634593605995178,
      "learning_rate": 8.953594176524111e-05,
      "loss": 1.5033,
      "step": 6170
    },
    {
      "epoch": 0.70284428743545,
      "grad_norm": 0.6374201774597168,
      "learning_rate": 8.919472247497725e-05,
      "loss": 1.4952,
      "step": 6180
    },
    {
      "epoch": 0.7039815759264458,
      "grad_norm": 0.6921415328979492,
      "learning_rate": 8.885350318471336e-05,
      "loss": 1.4935,
      "step": 6190
    },
    {
      "epoch": 0.7051188644174418,
      "grad_norm": 0.641864538192749,
      "learning_rate": 8.85122838944495e-05,
      "loss": 1.4923,
      "step": 6200
    },
    {
      "epoch": 0.7062561529084376,
      "grad_norm": 0.7255018353462219,
      "learning_rate": 8.817106460418561e-05,
      "loss": 1.5255,
      "step": 6210
    },
    {
      "epoch": 0.7073934413994335,
      "grad_norm": 0.6576597690582275,
      "learning_rate": 8.782984531392173e-05,
      "loss": 1.4756,
      "step": 6220
    },
    {
      "epoch": 0.7085307298904293,
      "grad_norm": 0.6678227186203003,
      "learning_rate": 8.748862602365787e-05,
      "loss": 1.54,
      "step": 6230
    },
    {
      "epoch": 0.7096680183814252,
      "grad_norm": 0.609615683555603,
      "learning_rate": 8.714740673339398e-05,
      "loss": 1.4955,
      "step": 6240
    },
    {
      "epoch": 0.7108053068724212,
      "grad_norm": 0.6619716882705688,
      "learning_rate": 8.680618744313012e-05,
      "loss": 1.5181,
      "step": 6250
    },
    {
      "epoch": 0.711942595363417,
      "grad_norm": 0.6573576331138611,
      "learning_rate": 8.646496815286623e-05,
      "loss": 1.5493,
      "step": 6260
    },
    {
      "epoch": 0.7130798838544129,
      "grad_norm": 0.6059869527816772,
      "learning_rate": 8.612374886260237e-05,
      "loss": 1.4727,
      "step": 6270
    },
    {
      "epoch": 0.7142171723454087,
      "grad_norm": 0.5762116312980652,
      "learning_rate": 8.578252957233848e-05,
      "loss": 1.5416,
      "step": 6280
    },
    {
      "epoch": 0.7153544608364046,
      "grad_norm": 0.6169584393501282,
      "learning_rate": 8.54413102820746e-05,
      "loss": 1.4979,
      "step": 6290
    },
    {
      "epoch": 0.7164917493274005,
      "grad_norm": 0.671549916267395,
      "learning_rate": 8.510009099181073e-05,
      "loss": 1.4626,
      "step": 6300
    },
    {
      "epoch": 0.7176290378183964,
      "grad_norm": 0.5688862204551697,
      "learning_rate": 8.475887170154685e-05,
      "loss": 1.5533,
      "step": 6310
    },
    {
      "epoch": 0.7187663263093922,
      "grad_norm": 0.6706468462944031,
      "learning_rate": 8.441765241128299e-05,
      "loss": 1.4957,
      "step": 6320
    },
    {
      "epoch": 0.7199036148003881,
      "grad_norm": 0.6728705167770386,
      "learning_rate": 8.40764331210191e-05,
      "loss": 1.4949,
      "step": 6330
    },
    {
      "epoch": 0.7210409032913839,
      "grad_norm": 0.6076549887657166,
      "learning_rate": 8.373521383075524e-05,
      "loss": 1.5196,
      "step": 6340
    },
    {
      "epoch": 0.7221781917823799,
      "grad_norm": 0.6862390637397766,
      "learning_rate": 8.339399454049135e-05,
      "loss": 1.5313,
      "step": 6350
    },
    {
      "epoch": 0.7233154802733757,
      "grad_norm": 0.6606104373931885,
      "learning_rate": 8.305277525022746e-05,
      "loss": 1.4825,
      "step": 6360
    },
    {
      "epoch": 0.7244527687643716,
      "grad_norm": 0.6160237193107605,
      "learning_rate": 8.27115559599636e-05,
      "loss": 1.4792,
      "step": 6370
    },
    {
      "epoch": 0.7255900572553675,
      "grad_norm": 0.5905904173851013,
      "learning_rate": 8.237033666969971e-05,
      "loss": 1.5357,
      "step": 6380
    },
    {
      "epoch": 0.7267273457463633,
      "grad_norm": 0.5810151100158691,
      "learning_rate": 8.202911737943585e-05,
      "loss": 1.4903,
      "step": 6390
    },
    {
      "epoch": 0.7278646342373593,
      "grad_norm": 0.6367857456207275,
      "learning_rate": 8.168789808917196e-05,
      "loss": 1.4961,
      "step": 6400
    },
    {
      "epoch": 0.7290019227283551,
      "grad_norm": 0.6465632319450378,
      "learning_rate": 8.13466787989081e-05,
      "loss": 1.4823,
      "step": 6410
    },
    {
      "epoch": 0.730139211219351,
      "grad_norm": 0.6628455519676208,
      "learning_rate": 8.100545950864422e-05,
      "loss": 1.5191,
      "step": 6420
    },
    {
      "epoch": 0.7312764997103468,
      "grad_norm": 0.5965058207511902,
      "learning_rate": 8.066424021838033e-05,
      "loss": 1.5462,
      "step": 6430
    },
    {
      "epoch": 0.7324137882013427,
      "grad_norm": 0.6199477314949036,
      "learning_rate": 8.032302092811647e-05,
      "loss": 1.5631,
      "step": 6440
    },
    {
      "epoch": 0.7335510766923385,
      "grad_norm": 0.6626135110855103,
      "learning_rate": 7.998180163785258e-05,
      "loss": 1.5291,
      "step": 6450
    },
    {
      "epoch": 0.7346883651833345,
      "grad_norm": 0.6029537916183472,
      "learning_rate": 7.964058234758872e-05,
      "loss": 1.5239,
      "step": 6460
    },
    {
      "epoch": 0.7358256536743303,
      "grad_norm": 0.6511523127555847,
      "learning_rate": 7.929936305732483e-05,
      "loss": 1.5084,
      "step": 6470
    },
    {
      "epoch": 0.7369629421653262,
      "grad_norm": 0.6880362033843994,
      "learning_rate": 7.895814376706094e-05,
      "loss": 1.5239,
      "step": 6480
    },
    {
      "epoch": 0.7381002306563221,
      "grad_norm": 0.6454867720603943,
      "learning_rate": 7.861692447679708e-05,
      "loss": 1.5215,
      "step": 6490
    },
    {
      "epoch": 0.739237519147318,
      "grad_norm": 0.640092134475708,
      "learning_rate": 7.82757051865332e-05,
      "loss": 1.5165,
      "step": 6500
    },
    {
      "epoch": 0.7403748076383139,
      "grad_norm": 0.6461473107337952,
      "learning_rate": 7.793448589626934e-05,
      "loss": 1.545,
      "step": 6510
    },
    {
      "epoch": 0.7415120961293097,
      "grad_norm": 0.5951345562934875,
      "learning_rate": 7.759326660600545e-05,
      "loss": 1.5139,
      "step": 6520
    },
    {
      "epoch": 0.7426493846203056,
      "grad_norm": 0.6593740582466125,
      "learning_rate": 7.725204731574159e-05,
      "loss": 1.5303,
      "step": 6530
    },
    {
      "epoch": 0.7437866731113014,
      "grad_norm": 0.6003733277320862,
      "learning_rate": 7.69108280254777e-05,
      "loss": 1.4617,
      "step": 6540
    },
    {
      "epoch": 0.7449239616022973,
      "grad_norm": 0.6227840185165405,
      "learning_rate": 7.656960873521381e-05,
      "loss": 1.5003,
      "step": 6550
    },
    {
      "epoch": 0.7460612500932932,
      "grad_norm": 0.646176278591156,
      "learning_rate": 7.622838944494995e-05,
      "loss": 1.4915,
      "step": 6560
    },
    {
      "epoch": 0.7471985385842891,
      "grad_norm": 0.5889680981636047,
      "learning_rate": 7.588717015468606e-05,
      "loss": 1.5153,
      "step": 6570
    },
    {
      "epoch": 0.7483358270752849,
      "grad_norm": 0.5677503347396851,
      "learning_rate": 7.55459508644222e-05,
      "loss": 1.5339,
      "step": 6580
    },
    {
      "epoch": 0.7494731155662808,
      "grad_norm": 0.5852164626121521,
      "learning_rate": 7.520473157415832e-05,
      "loss": 1.5425,
      "step": 6590
    },
    {
      "epoch": 0.7506104040572766,
      "grad_norm": 0.6308192014694214,
      "learning_rate": 7.486351228389444e-05,
      "loss": 1.4864,
      "step": 6600
    },
    {
      "epoch": 0.7517476925482726,
      "grad_norm": 0.646756649017334,
      "learning_rate": 7.452229299363057e-05,
      "loss": 1.4582,
      "step": 6610
    },
    {
      "epoch": 0.7528849810392685,
      "grad_norm": 0.539513111114502,
      "learning_rate": 7.418107370336669e-05,
      "loss": 1.4667,
      "step": 6620
    },
    {
      "epoch": 0.7540222695302643,
      "grad_norm": 0.630966067314148,
      "learning_rate": 7.383985441310282e-05,
      "loss": 1.4761,
      "step": 6630
    },
    {
      "epoch": 0.7551595580212602,
      "grad_norm": 0.684889554977417,
      "learning_rate": 7.349863512283894e-05,
      "loss": 1.5286,
      "step": 6640
    },
    {
      "epoch": 0.756296846512256,
      "grad_norm": 0.6063655614852905,
      "learning_rate": 7.315741583257506e-05,
      "loss": 1.5066,
      "step": 6650
    },
    {
      "epoch": 0.757434135003252,
      "grad_norm": 0.5644472241401672,
      "learning_rate": 7.281619654231118e-05,
      "loss": 1.4836,
      "step": 6660
    },
    {
      "epoch": 0.7585714234942478,
      "grad_norm": 0.6454351544380188,
      "learning_rate": 7.247497725204731e-05,
      "loss": 1.5034,
      "step": 6670
    },
    {
      "epoch": 0.7597087119852437,
      "grad_norm": 0.6333650350570679,
      "learning_rate": 7.213375796178343e-05,
      "loss": 1.5049,
      "step": 6680
    },
    {
      "epoch": 0.7608460004762395,
      "grad_norm": 0.6491120457649231,
      "learning_rate": 7.179253867151956e-05,
      "loss": 1.4834,
      "step": 6690
    },
    {
      "epoch": 0.7619832889672354,
      "grad_norm": 0.6369915008544922,
      "learning_rate": 7.145131938125569e-05,
      "loss": 1.5303,
      "step": 6700
    },
    {
      "epoch": 0.7631205774582313,
      "grad_norm": 0.5943610668182373,
      "learning_rate": 7.11101000909918e-05,
      "loss": 1.5021,
      "step": 6710
    },
    {
      "epoch": 0.7642578659492272,
      "grad_norm": 0.5773643851280212,
      "learning_rate": 7.076888080072792e-05,
      "loss": 1.5355,
      "step": 6720
    },
    {
      "epoch": 0.7653951544402231,
      "grad_norm": 0.589083194732666,
      "learning_rate": 7.042766151046405e-05,
      "loss": 1.4956,
      "step": 6730
    },
    {
      "epoch": 0.7665324429312189,
      "grad_norm": 0.6118695735931396,
      "learning_rate": 7.008644222020018e-05,
      "loss": 1.5085,
      "step": 6740
    },
    {
      "epoch": 0.7676697314222148,
      "grad_norm": 0.6230352520942688,
      "learning_rate": 6.97452229299363e-05,
      "loss": 1.5036,
      "step": 6750
    },
    {
      "epoch": 0.7688070199132107,
      "grad_norm": 0.5953570008277893,
      "learning_rate": 6.940400363967243e-05,
      "loss": 1.4805,
      "step": 6760
    },
    {
      "epoch": 0.7699443084042066,
      "grad_norm": 0.6137364506721497,
      "learning_rate": 6.906278434940855e-05,
      "loss": 1.5177,
      "step": 6770
    },
    {
      "epoch": 0.7710815968952024,
      "grad_norm": 0.6271656155586243,
      "learning_rate": 6.872156505914467e-05,
      "loss": 1.5076,
      "step": 6780
    },
    {
      "epoch": 0.7722188853861983,
      "grad_norm": 0.5838236808776855,
      "learning_rate": 6.838034576888079e-05,
      "loss": 1.5045,
      "step": 6790
    },
    {
      "epoch": 0.7733561738771941,
      "grad_norm": 0.6345523595809937,
      "learning_rate": 6.803912647861692e-05,
      "loss": 1.5056,
      "step": 6800
    },
    {
      "epoch": 0.77449346236819,
      "grad_norm": 0.6182921528816223,
      "learning_rate": 6.769790718835304e-05,
      "loss": 1.4332,
      "step": 6810
    },
    {
      "epoch": 0.7756307508591859,
      "grad_norm": 0.5931599736213684,
      "learning_rate": 6.735668789808917e-05,
      "loss": 1.469,
      "step": 6820
    },
    {
      "epoch": 0.7767680393501818,
      "grad_norm": 0.5825397968292236,
      "learning_rate": 6.70154686078253e-05,
      "loss": 1.4354,
      "step": 6830
    },
    {
      "epoch": 0.7779053278411776,
      "grad_norm": 0.6580415964126587,
      "learning_rate": 6.667424931756141e-05,
      "loss": 1.5079,
      "step": 6840
    },
    {
      "epoch": 0.7790426163321735,
      "grad_norm": 0.5747393369674683,
      "learning_rate": 6.633303002729753e-05,
      "loss": 1.5143,
      "step": 6850
    },
    {
      "epoch": 0.7801799048231695,
      "grad_norm": 0.603344738483429,
      "learning_rate": 6.599181073703366e-05,
      "loss": 1.5174,
      "step": 6860
    },
    {
      "epoch": 0.7813171933141653,
      "grad_norm": 0.5816549062728882,
      "learning_rate": 6.565059144676978e-05,
      "loss": 1.4821,
      "step": 6870
    },
    {
      "epoch": 0.7824544818051612,
      "grad_norm": 0.5977742075920105,
      "learning_rate": 6.530937215650591e-05,
      "loss": 1.5076,
      "step": 6880
    },
    {
      "epoch": 0.783591770296157,
      "grad_norm": 0.5789979696273804,
      "learning_rate": 6.496815286624204e-05,
      "loss": 1.4747,
      "step": 6890
    },
    {
      "epoch": 0.7847290587871529,
      "grad_norm": 0.578762948513031,
      "learning_rate": 6.462693357597816e-05,
      "loss": 1.4869,
      "step": 6900
    },
    {
      "epoch": 0.7858663472781487,
      "grad_norm": 0.6360827684402466,
      "learning_rate": 6.428571428571427e-05,
      "loss": 1.4928,
      "step": 6910
    },
    {
      "epoch": 0.7870036357691447,
      "grad_norm": 0.6459407806396484,
      "learning_rate": 6.39444949954504e-05,
      "loss": 1.5148,
      "step": 6920
    },
    {
      "epoch": 0.7881409242601405,
      "grad_norm": 0.6266627311706543,
      "learning_rate": 6.360327570518653e-05,
      "loss": 1.5026,
      "step": 6930
    },
    {
      "epoch": 0.7892782127511364,
      "grad_norm": 0.6901500821113586,
      "learning_rate": 6.326205641492265e-05,
      "loss": 1.4983,
      "step": 6940
    },
    {
      "epoch": 0.7904155012421322,
      "grad_norm": 0.62278151512146,
      "learning_rate": 6.292083712465878e-05,
      "loss": 1.4785,
      "step": 6950
    },
    {
      "epoch": 0.7915527897331281,
      "grad_norm": 1.1501762866973877,
      "learning_rate": 6.25796178343949e-05,
      "loss": 1.4979,
      "step": 6960
    },
    {
      "epoch": 0.7926900782241241,
      "grad_norm": 0.6684385538101196,
      "learning_rate": 6.223839854413103e-05,
      "loss": 1.4683,
      "step": 6970
    },
    {
      "epoch": 0.7938273667151199,
      "grad_norm": 0.6658669710159302,
      "learning_rate": 6.189717925386714e-05,
      "loss": 1.5339,
      "step": 6980
    },
    {
      "epoch": 0.7949646552061158,
      "grad_norm": 0.5909178256988525,
      "learning_rate": 6.155595996360327e-05,
      "loss": 1.4717,
      "step": 6990
    },
    {
      "epoch": 0.7961019436971116,
      "grad_norm": 0.6087794303894043,
      "learning_rate": 6.12147406733394e-05,
      "loss": 1.5334,
      "step": 7000
    },
    {
      "epoch": 0.7972392321881075,
      "grad_norm": 0.589637041091919,
      "learning_rate": 6.087352138307552e-05,
      "loss": 1.4909,
      "step": 7010
    },
    {
      "epoch": 0.7983765206791034,
      "grad_norm": 0.6076578497886658,
      "learning_rate": 6.0532302092811645e-05,
      "loss": 1.5409,
      "step": 7020
    },
    {
      "epoch": 0.7995138091700993,
      "grad_norm": 0.622524082660675,
      "learning_rate": 6.019108280254777e-05,
      "loss": 1.4882,
      "step": 7030
    },
    {
      "epoch": 0.8006510976610951,
      "grad_norm": 0.6636172533035278,
      "learning_rate": 5.984986351228388e-05,
      "loss": 1.5158,
      "step": 7040
    },
    {
      "epoch": 0.801788386152091,
      "grad_norm": 0.7471121549606323,
      "learning_rate": 5.950864422202001e-05,
      "loss": 1.4958,
      "step": 7050
    },
    {
      "epoch": 0.8029256746430868,
      "grad_norm": 0.6284568905830383,
      "learning_rate": 5.9167424931756135e-05,
      "loss": 1.4944,
      "step": 7060
    },
    {
      "epoch": 0.8040629631340828,
      "grad_norm": 0.6227210164070129,
      "learning_rate": 5.882620564149226e-05,
      "loss": 1.5059,
      "step": 7070
    },
    {
      "epoch": 0.8052002516250786,
      "grad_norm": 0.6791685819625854,
      "learning_rate": 5.8484986351228386e-05,
      "loss": 1.455,
      "step": 7080
    },
    {
      "epoch": 0.8063375401160745,
      "grad_norm": 0.7612029314041138,
      "learning_rate": 5.814376706096451e-05,
      "loss": 1.4718,
      "step": 7090
    },
    {
      "epoch": 0.8074748286070704,
      "grad_norm": 0.6607879400253296,
      "learning_rate": 5.780254777070064e-05,
      "loss": 1.4756,
      "step": 7100
    },
    {
      "epoch": 0.8086121170980662,
      "grad_norm": 0.6742188930511475,
      "learning_rate": 5.746132848043675e-05,
      "loss": 1.5075,
      "step": 7110
    },
    {
      "epoch": 0.8097494055890622,
      "grad_norm": 0.6062257885932922,
      "learning_rate": 5.7120109190172876e-05,
      "loss": 1.5228,
      "step": 7120
    },
    {
      "epoch": 0.810886694080058,
      "grad_norm": 0.6478132605552673,
      "learning_rate": 5.6778889899909e-05,
      "loss": 1.4879,
      "step": 7130
    },
    {
      "epoch": 0.8120239825710539,
      "grad_norm": 0.5751350522041321,
      "learning_rate": 5.643767060964513e-05,
      "loss": 1.4872,
      "step": 7140
    },
    {
      "epoch": 0.8131612710620497,
      "grad_norm": 0.5986314415931702,
      "learning_rate": 5.6096451319381254e-05,
      "loss": 1.4674,
      "step": 7150
    },
    {
      "epoch": 0.8142985595530456,
      "grad_norm": 0.6183854341506958,
      "learning_rate": 5.575523202911738e-05,
      "loss": 1.5055,
      "step": 7160
    },
    {
      "epoch": 0.8154358480440415,
      "grad_norm": 0.6893613934516907,
      "learning_rate": 5.541401273885349e-05,
      "loss": 1.5202,
      "step": 7170
    },
    {
      "epoch": 0.8165731365350374,
      "grad_norm": 0.6889755129814148,
      "learning_rate": 5.507279344858962e-05,
      "loss": 1.538,
      "step": 7180
    },
    {
      "epoch": 0.8177104250260332,
      "grad_norm": 0.6010313630104065,
      "learning_rate": 5.4731574158325744e-05,
      "loss": 1.4446,
      "step": 7190
    },
    {
      "epoch": 0.8188477135170291,
      "grad_norm": 0.6133768558502197,
      "learning_rate": 5.439035486806187e-05,
      "loss": 1.4935,
      "step": 7200
    },
    {
      "epoch": 0.819985002008025,
      "grad_norm": 0.6484007835388184,
      "learning_rate": 5.4049135577797995e-05,
      "loss": 1.5096,
      "step": 7210
    },
    {
      "epoch": 0.8211222904990209,
      "grad_norm": 0.679431140422821,
      "learning_rate": 5.370791628753412e-05,
      "loss": 1.5569,
      "step": 7220
    },
    {
      "epoch": 0.8222595789900168,
      "grad_norm": 0.702022135257721,
      "learning_rate": 5.336669699727025e-05,
      "loss": 1.5437,
      "step": 7230
    },
    {
      "epoch": 0.8233968674810126,
      "grad_norm": 0.6469970345497131,
      "learning_rate": 5.302547770700636e-05,
      "loss": 1.4533,
      "step": 7240
    },
    {
      "epoch": 0.8245341559720085,
      "grad_norm": 0.6301332712173462,
      "learning_rate": 5.2684258416742485e-05,
      "loss": 1.5239,
      "step": 7250
    },
    {
      "epoch": 0.8256714444630043,
      "grad_norm": 0.688488245010376,
      "learning_rate": 5.234303912647861e-05,
      "loss": 1.4914,
      "step": 7260
    },
    {
      "epoch": 0.8268087329540003,
      "grad_norm": 0.5752722024917603,
      "learning_rate": 5.200181983621474e-05,
      "loss": 1.5043,
      "step": 7270
    },
    {
      "epoch": 0.8279460214449961,
      "grad_norm": 0.6557264924049377,
      "learning_rate": 5.166060054595086e-05,
      "loss": 1.5556,
      "step": 7280
    },
    {
      "epoch": 0.829083309935992,
      "grad_norm": 0.6246957182884216,
      "learning_rate": 5.131938125568699e-05,
      "loss": 1.4882,
      "step": 7290
    },
    {
      "epoch": 0.8302205984269878,
      "grad_norm": 0.6729506850242615,
      "learning_rate": 5.0978161965423114e-05,
      "loss": 1.5573,
      "step": 7300
    },
    {
      "epoch": 0.8313578869179837,
      "grad_norm": 0.6704106330871582,
      "learning_rate": 5.0636942675159226e-05,
      "loss": 1.511,
      "step": 7310
    },
    {
      "epoch": 0.8324951754089795,
      "grad_norm": 0.5889140963554382,
      "learning_rate": 5.029572338489535e-05,
      "loss": 1.4423,
      "step": 7320
    },
    {
      "epoch": 0.8336324638999755,
      "grad_norm": 0.6719682812690735,
      "learning_rate": 4.995450409463148e-05,
      "loss": 1.4924,
      "step": 7330
    },
    {
      "epoch": 0.8347697523909714,
      "grad_norm": 0.6297319531440735,
      "learning_rate": 4.9613284804367604e-05,
      "loss": 1.521,
      "step": 7340
    },
    {
      "epoch": 0.8359070408819672,
      "grad_norm": 0.6421142816543579,
      "learning_rate": 4.927206551410373e-05,
      "loss": 1.4766,
      "step": 7350
    },
    {
      "epoch": 0.8370443293729631,
      "grad_norm": 0.6492081880569458,
      "learning_rate": 4.8930846223839856e-05,
      "loss": 1.4734,
      "step": 7360
    },
    {
      "epoch": 0.838181617863959,
      "grad_norm": 0.6667419672012329,
      "learning_rate": 4.858962693357597e-05,
      "loss": 1.5453,
      "step": 7370
    },
    {
      "epoch": 0.8393189063549549,
      "grad_norm": 0.660275399684906,
      "learning_rate": 4.8248407643312094e-05,
      "loss": 1.4826,
      "step": 7380
    },
    {
      "epoch": 0.8404561948459507,
      "grad_norm": 0.6537889242172241,
      "learning_rate": 4.790718835304822e-05,
      "loss": 1.4808,
      "step": 7390
    },
    {
      "epoch": 0.8415934833369466,
      "grad_norm": 0.6116306185722351,
      "learning_rate": 4.7565969062784345e-05,
      "loss": 1.5006,
      "step": 7400
    },
    {
      "epoch": 0.8427307718279424,
      "grad_norm": 0.6152806282043457,
      "learning_rate": 4.722474977252047e-05,
      "loss": 1.5164,
      "step": 7410
    },
    {
      "epoch": 0.8438680603189384,
      "grad_norm": 0.6203017234802246,
      "learning_rate": 4.68835304822566e-05,
      "loss": 1.4558,
      "step": 7420
    },
    {
      "epoch": 0.8450053488099342,
      "grad_norm": 0.6335179209709167,
      "learning_rate": 4.654231119199272e-05,
      "loss": 1.4836,
      "step": 7430
    },
    {
      "epoch": 0.8461426373009301,
      "grad_norm": 0.6671952605247498,
      "learning_rate": 4.6201091901728835e-05,
      "loss": 1.5088,
      "step": 7440
    },
    {
      "epoch": 0.847279925791926,
      "grad_norm": 0.5811948180198669,
      "learning_rate": 4.585987261146496e-05,
      "loss": 1.4833,
      "step": 7450
    },
    {
      "epoch": 0.8484172142829218,
      "grad_norm": 0.6179354786872864,
      "learning_rate": 4.551865332120109e-05,
      "loss": 1.5666,
      "step": 7460
    },
    {
      "epoch": 0.8495545027739178,
      "grad_norm": 0.623904824256897,
      "learning_rate": 4.517743403093721e-05,
      "loss": 1.4979,
      "step": 7470
    },
    {
      "epoch": 0.8506917912649136,
      "grad_norm": 0.6730799674987793,
      "learning_rate": 4.483621474067334e-05,
      "loss": 1.4569,
      "step": 7480
    },
    {
      "epoch": 0.8518290797559095,
      "grad_norm": 0.6519535183906555,
      "learning_rate": 4.4494995450409464e-05,
      "loss": 1.4988,
      "step": 7490
    },
    {
      "epoch": 0.8529663682469053,
      "grad_norm": 0.6943574547767639,
      "learning_rate": 4.415377616014558e-05,
      "loss": 1.5415,
      "step": 7500
    },
    {
      "epoch": 0.8541036567379012,
      "grad_norm": 0.68156898021698,
      "learning_rate": 4.38125568698817e-05,
      "loss": 1.5056,
      "step": 7510
    },
    {
      "epoch": 0.855240945228897,
      "grad_norm": 0.5759136080741882,
      "learning_rate": 4.347133757961783e-05,
      "loss": 1.5369,
      "step": 7520
    },
    {
      "epoch": 0.856378233719893,
      "grad_norm": 0.5659081935882568,
      "learning_rate": 4.3130118289353954e-05,
      "loss": 1.4345,
      "step": 7530
    },
    {
      "epoch": 0.8575155222108888,
      "grad_norm": 0.6519301533699036,
      "learning_rate": 4.278889899909008e-05,
      "loss": 1.5161,
      "step": 7540
    },
    {
      "epoch": 0.8586528107018847,
      "grad_norm": 0.691611111164093,
      "learning_rate": 4.2447679708826206e-05,
      "loss": 1.537,
      "step": 7550
    },
    {
      "epoch": 0.8597900991928805,
      "grad_norm": 0.6266446113586426,
      "learning_rate": 4.210646041856233e-05,
      "loss": 1.5222,
      "step": 7560
    },
    {
      "epoch": 0.8609273876838764,
      "grad_norm": 0.618621826171875,
      "learning_rate": 4.1765241128298444e-05,
      "loss": 1.5073,
      "step": 7570
    },
    {
      "epoch": 0.8620646761748724,
      "grad_norm": 0.5979098081588745,
      "learning_rate": 4.142402183803457e-05,
      "loss": 1.4718,
      "step": 7580
    },
    {
      "epoch": 0.8632019646658682,
      "grad_norm": 0.601536750793457,
      "learning_rate": 4.1082802547770696e-05,
      "loss": 1.5133,
      "step": 7590
    },
    {
      "epoch": 0.8643392531568641,
      "grad_norm": 0.6447013020515442,
      "learning_rate": 4.074158325750682e-05,
      "loss": 1.5134,
      "step": 7600
    },
    {
      "epoch": 0.8654765416478599,
      "grad_norm": 0.610120415687561,
      "learning_rate": 4.040036396724295e-05,
      "loss": 1.5179,
      "step": 7610
    },
    {
      "epoch": 0.8666138301388558,
      "grad_norm": 0.6089101433753967,
      "learning_rate": 4.005914467697907e-05,
      "loss": 1.4511,
      "step": 7620
    },
    {
      "epoch": 0.8677511186298517,
      "grad_norm": 0.6199619770050049,
      "learning_rate": 3.9717925386715186e-05,
      "loss": 1.5626,
      "step": 7630
    },
    {
      "epoch": 0.8688884071208476,
      "grad_norm": 0.6319074034690857,
      "learning_rate": 3.937670609645131e-05,
      "loss": 1.45,
      "step": 7640
    },
    {
      "epoch": 0.8700256956118434,
      "grad_norm": 0.655691385269165,
      "learning_rate": 3.903548680618744e-05,
      "loss": 1.4753,
      "step": 7650
    },
    {
      "epoch": 0.8711629841028393,
      "grad_norm": 0.6420767307281494,
      "learning_rate": 3.869426751592356e-05,
      "loss": 1.5117,
      "step": 7660
    },
    {
      "epoch": 0.8723002725938351,
      "grad_norm": 0.6914390325546265,
      "learning_rate": 3.835304822565969e-05,
      "loss": 1.5174,
      "step": 7670
    },
    {
      "epoch": 0.8734375610848311,
      "grad_norm": 0.6460720896720886,
      "learning_rate": 3.8011828935395815e-05,
      "loss": 1.5226,
      "step": 7680
    },
    {
      "epoch": 0.874574849575827,
      "grad_norm": 0.5637558698654175,
      "learning_rate": 3.767060964513194e-05,
      "loss": 1.4669,
      "step": 7690
    },
    {
      "epoch": 0.8757121380668228,
      "grad_norm": 0.6301305294036865,
      "learning_rate": 3.732939035486806e-05,
      "loss": 1.5087,
      "step": 7700
    },
    {
      "epoch": 0.8768494265578187,
      "grad_norm": 0.6205446124076843,
      "learning_rate": 3.6988171064604185e-05,
      "loss": 1.5003,
      "step": 7710
    },
    {
      "epoch": 0.8779867150488145,
      "grad_norm": 0.5863721966743469,
      "learning_rate": 3.6646951774340305e-05,
      "loss": 1.4943,
      "step": 7720
    },
    {
      "epoch": 0.8791240035398105,
      "grad_norm": 0.6100584864616394,
      "learning_rate": 3.630573248407643e-05,
      "loss": 1.5249,
      "step": 7730
    },
    {
      "epoch": 0.8802612920308063,
      "grad_norm": 0.6738633513450623,
      "learning_rate": 3.5964513193812556e-05,
      "loss": 1.5036,
      "step": 7740
    },
    {
      "epoch": 0.8813985805218022,
      "grad_norm": 0.6278347373008728,
      "learning_rate": 3.5623293903548675e-05,
      "loss": 1.4942,
      "step": 7750
    },
    {
      "epoch": 0.882535869012798,
      "grad_norm": 0.6166554093360901,
      "learning_rate": 3.52820746132848e-05,
      "loss": 1.512,
      "step": 7760
    },
    {
      "epoch": 0.8836731575037939,
      "grad_norm": 0.6014090776443481,
      "learning_rate": 3.494085532302093e-05,
      "loss": 1.5369,
      "step": 7770
    },
    {
      "epoch": 0.8848104459947898,
      "grad_norm": 0.6391996145248413,
      "learning_rate": 3.4599636032757046e-05,
      "loss": 1.4822,
      "step": 7780
    },
    {
      "epoch": 0.8859477344857857,
      "grad_norm": 0.6110092997550964,
      "learning_rate": 3.425841674249317e-05,
      "loss": 1.4819,
      "step": 7790
    },
    {
      "epoch": 0.8870850229767815,
      "grad_norm": 0.7006159424781799,
      "learning_rate": 3.39171974522293e-05,
      "loss": 1.5055,
      "step": 7800
    },
    {
      "epoch": 0.8882223114677774,
      "grad_norm": 0.6114333271980286,
      "learning_rate": 3.3575978161965424e-05,
      "loss": 1.4766,
      "step": 7810
    },
    {
      "epoch": 0.8893595999587733,
      "grad_norm": 0.6285127401351929,
      "learning_rate": 3.323475887170154e-05,
      "loss": 1.5248,
      "step": 7820
    },
    {
      "epoch": 0.8904968884497692,
      "grad_norm": 0.6148408651351929,
      "learning_rate": 3.289353958143767e-05,
      "loss": 1.4971,
      "step": 7830
    },
    {
      "epoch": 0.8916341769407651,
      "grad_norm": 0.6361715197563171,
      "learning_rate": 3.2552320291173794e-05,
      "loss": 1.5393,
      "step": 7840
    },
    {
      "epoch": 0.8927714654317609,
      "grad_norm": 0.6036791801452637,
      "learning_rate": 3.221110100090991e-05,
      "loss": 1.5699,
      "step": 7850
    },
    {
      "epoch": 0.8939087539227568,
      "grad_norm": 0.662703812122345,
      "learning_rate": 3.186988171064604e-05,
      "loss": 1.5025,
      "step": 7860
    },
    {
      "epoch": 0.8950460424137526,
      "grad_norm": 0.6216747760772705,
      "learning_rate": 3.1528662420382165e-05,
      "loss": 1.5047,
      "step": 7870
    },
    {
      "epoch": 0.8961833309047486,
      "grad_norm": 0.5701249837875366,
      "learning_rate": 3.1187443130118284e-05,
      "loss": 1.4654,
      "step": 7880
    },
    {
      "epoch": 0.8973206193957444,
      "grad_norm": 0.665240466594696,
      "learning_rate": 3.084622383985441e-05,
      "loss": 1.552,
      "step": 7890
    },
    {
      "epoch": 0.8984579078867403,
      "grad_norm": 0.6808103322982788,
      "learning_rate": 3.0505004549590536e-05,
      "loss": 1.4926,
      "step": 7900
    },
    {
      "epoch": 0.8995951963777361,
      "grad_norm": 0.5835521817207336,
      "learning_rate": 3.0163785259326655e-05,
      "loss": 1.5416,
      "step": 7910
    },
    {
      "epoch": 0.900732484868732,
      "grad_norm": 0.6667680144309998,
      "learning_rate": 2.982256596906278e-05,
      "loss": 1.5428,
      "step": 7920
    },
    {
      "epoch": 0.901869773359728,
      "grad_norm": 0.5944315195083618,
      "learning_rate": 2.9481346678798906e-05,
      "loss": 1.5105,
      "step": 7930
    },
    {
      "epoch": 0.9030070618507238,
      "grad_norm": 0.6070797443389893,
      "learning_rate": 2.9140127388535032e-05,
      "loss": 1.5057,
      "step": 7940
    },
    {
      "epoch": 0.9041443503417197,
      "grad_norm": 0.6413010358810425,
      "learning_rate": 2.879890809827115e-05,
      "loss": 1.5328,
      "step": 7950
    },
    {
      "epoch": 0.9052816388327155,
      "grad_norm": 0.6885035037994385,
      "learning_rate": 2.8457688808007277e-05,
      "loss": 1.4684,
      "step": 7960
    },
    {
      "epoch": 0.9064189273237114,
      "grad_norm": 0.6443766355514526,
      "learning_rate": 2.8116469517743403e-05,
      "loss": 1.483,
      "step": 7970
    },
    {
      "epoch": 0.9075562158147072,
      "grad_norm": 0.6498932242393494,
      "learning_rate": 2.7775250227479522e-05,
      "loss": 1.5112,
      "step": 7980
    },
    {
      "epoch": 0.9086935043057032,
      "grad_norm": 0.6372060775756836,
      "learning_rate": 2.7434030937215648e-05,
      "loss": 1.5189,
      "step": 7990
    },
    {
      "epoch": 0.909830792796699,
      "grad_norm": 0.6415179967880249,
      "learning_rate": 2.7092811646951774e-05,
      "loss": 1.4639,
      "step": 8000
    },
    {
      "epoch": 0.9109680812876949,
      "grad_norm": 0.6325279474258423,
      "learning_rate": 2.6751592356687893e-05,
      "loss": 1.4718,
      "step": 8010
    },
    {
      "epoch": 0.9121053697786907,
      "grad_norm": 0.6777604222297668,
      "learning_rate": 2.641037306642402e-05,
      "loss": 1.5498,
      "step": 8020
    },
    {
      "epoch": 0.9132426582696866,
      "grad_norm": 0.5904757380485535,
      "learning_rate": 2.6069153776160145e-05,
      "loss": 1.4858,
      "step": 8030
    },
    {
      "epoch": 0.9143799467606825,
      "grad_norm": 0.6257051825523376,
      "learning_rate": 2.572793448589627e-05,
      "loss": 1.5044,
      "step": 8040
    },
    {
      "epoch": 0.9155172352516784,
      "grad_norm": 0.6970100402832031,
      "learning_rate": 2.538671519563239e-05,
      "loss": 1.4795,
      "step": 8050
    },
    {
      "epoch": 0.9166545237426743,
      "grad_norm": 0.6843002438545227,
      "learning_rate": 2.5045495905368515e-05,
      "loss": 1.4336,
      "step": 8060
    },
    {
      "epoch": 0.9177918122336701,
      "grad_norm": 0.6103501915931702,
      "learning_rate": 2.470427661510464e-05,
      "loss": 1.5092,
      "step": 8070
    },
    {
      "epoch": 0.918929100724666,
      "grad_norm": 0.6165107488632202,
      "learning_rate": 2.436305732484076e-05,
      "loss": 1.4744,
      "step": 8080
    },
    {
      "epoch": 0.9200663892156619,
      "grad_norm": 0.6776142120361328,
      "learning_rate": 2.4021838034576886e-05,
      "loss": 1.5688,
      "step": 8090
    },
    {
      "epoch": 0.9212036777066578,
      "grad_norm": 0.6154009103775024,
      "learning_rate": 2.3680618744313012e-05,
      "loss": 1.5445,
      "step": 8100
    },
    {
      "epoch": 0.9223409661976536,
      "grad_norm": 0.6201418042182922,
      "learning_rate": 2.333939945404913e-05,
      "loss": 1.5389,
      "step": 8110
    },
    {
      "epoch": 0.9234782546886495,
      "grad_norm": 0.6026250720024109,
      "learning_rate": 2.2998180163785257e-05,
      "loss": 1.4897,
      "step": 8120
    },
    {
      "epoch": 0.9246155431796453,
      "grad_norm": 0.6464824676513672,
      "learning_rate": 2.2656960873521383e-05,
      "loss": 1.4565,
      "step": 8130
    },
    {
      "epoch": 0.9257528316706413,
      "grad_norm": 0.600256085395813,
      "learning_rate": 2.23157415832575e-05,
      "loss": 1.4929,
      "step": 8140
    },
    {
      "epoch": 0.9268901201616371,
      "grad_norm": 0.6231455206871033,
      "learning_rate": 2.1974522292993627e-05,
      "loss": 1.4893,
      "step": 8150
    },
    {
      "epoch": 0.928027408652633,
      "grad_norm": 0.6315721273422241,
      "learning_rate": 2.1633303002729753e-05,
      "loss": 1.5534,
      "step": 8160
    },
    {
      "epoch": 0.9291646971436289,
      "grad_norm": 0.6155976057052612,
      "learning_rate": 2.129208371246588e-05,
      "loss": 1.5196,
      "step": 8170
    },
    {
      "epoch": 0.9303019856346247,
      "grad_norm": 0.6529756188392639,
      "learning_rate": 2.0950864422201998e-05,
      "loss": 1.4945,
      "step": 8180
    },
    {
      "epoch": 0.9314392741256207,
      "grad_norm": 0.6467315554618835,
      "learning_rate": 2.0609645131938124e-05,
      "loss": 1.4903,
      "step": 8190
    },
    {
      "epoch": 0.9325765626166165,
      "grad_norm": 0.6528888940811157,
      "learning_rate": 2.026842584167425e-05,
      "loss": 1.4929,
      "step": 8200
    },
    {
      "epoch": 0.9337138511076124,
      "grad_norm": 0.6579816937446594,
      "learning_rate": 1.992720655141037e-05,
      "loss": 1.5465,
      "step": 8210
    },
    {
      "epoch": 0.9348511395986082,
      "grad_norm": 0.7275859117507935,
      "learning_rate": 1.9585987261146495e-05,
      "loss": 1.5082,
      "step": 8220
    },
    {
      "epoch": 0.9359884280896041,
      "grad_norm": 0.6436865925788879,
      "learning_rate": 1.924476797088262e-05,
      "loss": 1.5233,
      "step": 8230
    },
    {
      "epoch": 0.9371257165806,
      "grad_norm": 0.6402204632759094,
      "learning_rate": 1.890354868061874e-05,
      "loss": 1.5089,
      "step": 8240
    },
    {
      "epoch": 0.9382630050715959,
      "grad_norm": 0.592654824256897,
      "learning_rate": 1.8562329390354866e-05,
      "loss": 1.4416,
      "step": 8250
    },
    {
      "epoch": 0.9394002935625917,
      "grad_norm": 0.6745343208312988,
      "learning_rate": 1.822111010009099e-05,
      "loss": 1.4988,
      "step": 8260
    },
    {
      "epoch": 0.9405375820535876,
      "grad_norm": 0.6021124720573425,
      "learning_rate": 1.7879890809827114e-05,
      "loss": 1.508,
      "step": 8270
    },
    {
      "epoch": 0.9416748705445834,
      "grad_norm": 0.6121383905410767,
      "learning_rate": 1.7538671519563236e-05,
      "loss": 1.5199,
      "step": 8280
    },
    {
      "epoch": 0.9428121590355794,
      "grad_norm": 0.6051508784294128,
      "learning_rate": 1.7197452229299362e-05,
      "loss": 1.496,
      "step": 8290
    },
    {
      "epoch": 0.9439494475265753,
      "grad_norm": 0.6513348817825317,
      "learning_rate": 1.6856232939035485e-05,
      "loss": 1.4903,
      "step": 8300
    },
    {
      "epoch": 0.9450867360175711,
      "grad_norm": 0.662132978439331,
      "learning_rate": 1.651501364877161e-05,
      "loss": 1.4725,
      "step": 8310
    },
    {
      "epoch": 0.946224024508567,
      "grad_norm": 0.5974112749099731,
      "learning_rate": 1.6173794358507733e-05,
      "loss": 1.5046,
      "step": 8320
    },
    {
      "epoch": 0.9473613129995628,
      "grad_norm": 0.6738801002502441,
      "learning_rate": 1.5832575068243855e-05,
      "loss": 1.459,
      "step": 8330
    },
    {
      "epoch": 0.9484986014905588,
      "grad_norm": 0.6375149488449097,
      "learning_rate": 1.549135577797998e-05,
      "loss": 1.4818,
      "step": 8340
    },
    {
      "epoch": 0.9496358899815546,
      "grad_norm": 0.702319324016571,
      "learning_rate": 1.5184258416742492e-05,
      "loss": 1.4767,
      "step": 8350
    },
    {
      "epoch": 0.9507731784725505,
      "grad_norm": 0.5916227698326111,
      "learning_rate": 1.4843039126478614e-05,
      "loss": 1.4698,
      "step": 8360
    },
    {
      "epoch": 0.9519104669635463,
      "grad_norm": 0.6174399256706238,
      "learning_rate": 1.450181983621474e-05,
      "loss": 1.4907,
      "step": 8370
    },
    {
      "epoch": 0.9530477554545422,
      "grad_norm": 0.6070659756660461,
      "learning_rate": 1.4160600545950862e-05,
      "loss": 1.456,
      "step": 8380
    },
    {
      "epoch": 0.954185043945538,
      "grad_norm": 0.6437009572982788,
      "learning_rate": 1.3819381255686988e-05,
      "loss": 1.4381,
      "step": 8390
    },
    {
      "epoch": 0.955322332436534,
      "grad_norm": 0.5890575051307678,
      "learning_rate": 1.347816196542311e-05,
      "loss": 1.4775,
      "step": 8400
    },
    {
      "epoch": 0.9564596209275299,
      "grad_norm": 0.6197596192359924,
      "learning_rate": 1.3136942675159233e-05,
      "loss": 1.5476,
      "step": 8410
    },
    {
      "epoch": 0.9575969094185257,
      "grad_norm": 0.6156983375549316,
      "learning_rate": 1.2795723384895359e-05,
      "loss": 1.4619,
      "step": 8420
    },
    {
      "epoch": 0.9587341979095216,
      "grad_norm": 0.6540002822875977,
      "learning_rate": 1.2454504094631481e-05,
      "loss": 1.4535,
      "step": 8430
    },
    {
      "epoch": 0.9598714864005174,
      "grad_norm": 0.5566866397857666,
      "learning_rate": 1.2113284804367607e-05,
      "loss": 1.4787,
      "step": 8440
    },
    {
      "epoch": 0.9610087748915134,
      "grad_norm": 0.6048468351364136,
      "learning_rate": 1.177206551410373e-05,
      "loss": 1.5222,
      "step": 8450
    },
    {
      "epoch": 0.9621460633825092,
      "grad_norm": 0.6539886593818665,
      "learning_rate": 1.1430846223839852e-05,
      "loss": 1.526,
      "step": 8460
    },
    {
      "epoch": 0.9632833518735051,
      "grad_norm": 0.6128603219985962,
      "learning_rate": 1.1089626933575978e-05,
      "loss": 1.4727,
      "step": 8470
    },
    {
      "epoch": 0.9644206403645009,
      "grad_norm": 0.6458943486213684,
      "learning_rate": 1.07484076433121e-05,
      "loss": 1.48,
      "step": 8480
    },
    {
      "epoch": 0.9655579288554968,
      "grad_norm": 0.6754756569862366,
      "learning_rate": 1.0407188353048226e-05,
      "loss": 1.5388,
      "step": 8490
    },
    {
      "epoch": 0.9666952173464927,
      "grad_norm": 0.5736780166625977,
      "learning_rate": 1.0065969062784349e-05,
      "loss": 1.5429,
      "step": 8500
    },
    {
      "epoch": 0.9678325058374886,
      "grad_norm": 0.7088466286659241,
      "learning_rate": 9.724749772520471e-06,
      "loss": 1.547,
      "step": 8510
    },
    {
      "epoch": 0.9689697943284844,
      "grad_norm": 0.5896206498146057,
      "learning_rate": 9.383530482256597e-06,
      "loss": 1.5014,
      "step": 8520
    },
    {
      "epoch": 0.9701070828194803,
      "grad_norm": 0.6146101951599121,
      "learning_rate": 9.04231119199272e-06,
      "loss": 1.4579,
      "step": 8530
    },
    {
      "epoch": 0.9712443713104763,
      "grad_norm": 0.6622234582901001,
      "learning_rate": 8.701091901728844e-06,
      "loss": 1.5314,
      "step": 8540
    },
    {
      "epoch": 0.9723816598014721,
      "grad_norm": 0.69915372133255,
      "learning_rate": 8.359872611464968e-06,
      "loss": 1.4904,
      "step": 8550
    },
    {
      "epoch": 0.973518948292468,
      "grad_norm": 0.6052011847496033,
      "learning_rate": 8.018653321201092e-06,
      "loss": 1.4771,
      "step": 8560
    },
    {
      "epoch": 0.9746562367834638,
      "grad_norm": 0.6603407263755798,
      "learning_rate": 7.677434030937214e-06,
      "loss": 1.5432,
      "step": 8570
    },
    {
      "epoch": 0.9757935252744597,
      "grad_norm": 0.6046708822250366,
      "learning_rate": 7.3362147406733385e-06,
      "loss": 1.5183,
      "step": 8580
    },
    {
      "epoch": 0.9769308137654555,
      "grad_norm": 0.5686598420143127,
      "learning_rate": 6.994995450409463e-06,
      "loss": 1.4472,
      "step": 8590
    },
    {
      "epoch": 0.9780681022564515,
      "grad_norm": 0.617220401763916,
      "learning_rate": 6.653776160145587e-06,
      "loss": 1.4657,
      "step": 8600
    },
    {
      "epoch": 0.9792053907474473,
      "grad_norm": 0.6130727529525757,
      "learning_rate": 6.312556869881709e-06,
      "loss": 1.4871,
      "step": 8610
    },
    {
      "epoch": 0.9803426792384432,
      "grad_norm": 0.6296908855438232,
      "learning_rate": 5.971337579617833e-06,
      "loss": 1.4953,
      "step": 8620
    },
    {
      "epoch": 0.981479967729439,
      "grad_norm": 0.6235052943229675,
      "learning_rate": 5.6301182893539576e-06,
      "loss": 1.5052,
      "step": 8630
    },
    {
      "epoch": 0.9826172562204349,
      "grad_norm": 0.6734328866004944,
      "learning_rate": 5.288898999090082e-06,
      "loss": 1.4714,
      "step": 8640
    },
    {
      "epoch": 0.9837545447114309,
      "grad_norm": 0.6519281268119812,
      "learning_rate": 4.947679708826206e-06,
      "loss": 1.5466,
      "step": 8650
    },
    {
      "epoch": 0.9848918332024267,
      "grad_norm": 0.6024752855300903,
      "learning_rate": 4.606460418562329e-06,
      "loss": 1.533,
      "step": 8660
    },
    {
      "epoch": 0.9860291216934226,
      "grad_norm": 0.6326128840446472,
      "learning_rate": 4.2652411282984524e-06,
      "loss": 1.5229,
      "step": 8670
    },
    {
      "epoch": 0.9871664101844184,
      "grad_norm": 0.6119192838668823,
      "learning_rate": 3.924021838034577e-06,
      "loss": 1.4788,
      "step": 8680
    },
    {
      "epoch": 0.9883036986754143,
      "grad_norm": 0.5988537669181824,
      "learning_rate": 3.5828025477707007e-06,
      "loss": 1.538,
      "step": 8690
    },
    {
      "epoch": 0.9894409871664102,
      "grad_norm": 0.6337285041809082,
      "learning_rate": 3.241583257506824e-06,
      "loss": 1.5333,
      "step": 8700
    },
    {
      "epoch": 0.9905782756574061,
      "grad_norm": 0.5812399983406067,
      "learning_rate": 2.900363967242948e-06,
      "loss": 1.5157,
      "step": 8710
    },
    {
      "epoch": 0.9917155641484019,
      "grad_norm": 0.6374207139015198,
      "learning_rate": 2.5591446769790715e-06,
      "loss": 1.5328,
      "step": 8720
    },
    {
      "epoch": 0.9928528526393978,
      "grad_norm": 0.6066370010375977,
      "learning_rate": 2.2179253867151956e-06,
      "loss": 1.4918,
      "step": 8730
    },
    {
      "epoch": 0.9939901411303936,
      "grad_norm": 0.658681333065033,
      "learning_rate": 1.8767060964513193e-06,
      "loss": 1.5019,
      "step": 8740
    },
    {
      "epoch": 0.9951274296213896,
      "grad_norm": 0.5949379205703735,
      "learning_rate": 1.535486806187443e-06,
      "loss": 1.4851,
      "step": 8750
    },
    {
      "epoch": 0.9962647181123854,
      "grad_norm": 0.5970675945281982,
      "learning_rate": 1.1942675159235668e-06,
      "loss": 1.515,
      "step": 8760
    },
    {
      "epoch": 0.9974020066033813,
      "grad_norm": 0.6207515597343445,
      "learning_rate": 8.530482256596905e-07,
      "loss": 1.4889,
      "step": 8770
    },
    {
      "epoch": 0.9985392950943772,
      "grad_norm": 0.6743981242179871,
      "learning_rate": 5.118289353958143e-07,
      "loss": 1.5158,
      "step": 8780
    },
    {
      "epoch": 0.999676583585373,
      "grad_norm": 0.6443230509757996,
      "learning_rate": 1.706096451319381e-07,
      "loss": 1.4841,
      "step": 8790
    }
  ],
  "logging_steps": 10,
  "max_steps": 8792,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5689607455960064e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
