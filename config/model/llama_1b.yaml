base_model_name: meta-llama/Llama-3.2-1B-Instruct
save_name: llama_1b
batch_size: 1
gradient_accumulation_step: 32
epoch: 1
learning_rate: 3e-4
load_in_8bit: true
optimizer: paged_adamw_8bit
