base_model_name: meta-llama/Llama-3.2-1B-Instruct
save_name: llama_lora_1b
is_lora: true
lora_alpha: 16
lora_rank: 8
lora_targets:
  - q_proj
  - v_proj
lora_dropout: 0.
batch_size: 1
gradient_accumulation_step: 32
epoch: 1
learning_rate: 3e-4
load_in_8bit: true
optimizer: paged_adamw_8bit
